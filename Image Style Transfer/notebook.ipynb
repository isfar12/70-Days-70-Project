{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "878a400a",
   "metadata": {},
   "source": [
    "# Neural Style Transfer - Deep Conceptual Understanding\n",
    "\n",
    "## What is Neural Style Transfer?\n",
    "\n",
    "**The Concept**: Neural Style Transfer is an optimization technique that uses deep learning to compose one image in the artistic style of another image. Imagine taking a photograph and recreating it as if it were painted by Van Gogh or Picasso.\n",
    "\n",
    "**The Problem We're Solving**: \n",
    "- Traditional image filters can only apply simple transformations (blur, sharpen, color shifts)\n",
    "- We want to capture the complex artistic \"essence\" of a painting and apply it to any photo\n",
    "- This requires understanding both **what** is in an image (content) and **how** it looks (style)\n",
    "\n",
    "**The Breakthrough Insight**: \n",
    "Convolutional Neural Networks (CNNs) naturally separate content from style at different layers:\n",
    "- **Early layers** (near input) capture low-level features: edges, colors, simple textures\n",
    "- **Deep layers** (near output) capture high-level features: object parts, shapes, semantic content\n",
    "\n",
    "**Why This Works**:\n",
    "1. **Content representation**: Deep layers know WHAT objects are in the image\n",
    "2. **Style representation**: The correlations between features (Gram matrix) capture HOW the image looks (textures, patterns, colors) regardless of WHAT is shown\n",
    "3. **Optimization**: We can adjust pixel values to match both content and style simultaneously\n",
    "\n",
    "## What We Need:\n",
    "\n",
    "1. **Pre-trained CNN**: VGG19 trained on millions of images - it already knows how to extract meaningful features\n",
    "2. **Content Image**: The photo whose structure we want to preserve\n",
    "3. **Style Image**: The artwork whose artistic style we want to copy\n",
    "4. **Generated Image**: Starting from content, we'll optimize its pixels to match style\n",
    "5. **Loss Functions**: Mathematical ways to measure content similarity and style similarity\n",
    "6. **Optimizer**: Algorithm to adjust pixels to minimize the total loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb85ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed58364a",
   "metadata": {},
   "source": [
    "## Step 1: Load Pre-trained VGG19 Model\n",
    "\n",
    "### What We Did:\n",
    "```python\n",
    "vgg_19 = models.vgg19(pretrained=True).features.eval()\n",
    "```\n",
    "\n",
    "### Why VGG19?\n",
    "1. **Deep architecture**: 19 layers means it learns hierarchical features\n",
    "2. **Well-studied**: We know exactly what each layer represents\n",
    "3. **Pre-trained**: Already trained on ImageNet (1.2M images, 1000 categories)\n",
    "4. **Good feature extractor**: Captures both low-level textures and high-level content\n",
    "\n",
    "### Line-by-Line Explanation:\n",
    "\n",
    "**`models.vgg19`**: Loads the VGG19 architecture\n",
    "- VGG19 has 16 convolutional layers + 3 fully connected layers\n",
    "- Named after Visual Geometry Group at Oxford\n",
    "\n",
    "**`pretrained=True`**: Downloads and loads weights trained on ImageNet\n",
    "- **Why?** We need a model that already understands images\n",
    "- Training from scratch would require millions of images and weeks of computation\n",
    "- Pre-trained weights encode knowledge about edges, textures, object parts\n",
    "\n",
    "**`.features`**: Extracts only the convolutional feature extraction part\n",
    "- **Why?** We don't need the classification layers (fully connected)\n",
    "- We only want the feature maps, not predictions of \"cat\" or \"dog\"\n",
    "- This gives us layers 0-28 (all conv + pooling layers)\n",
    "\n",
    "**`.eval()`**: Sets the model to evaluation mode\n",
    "- **Why?** Disables dropout and batch normalization training behavior\n",
    "- We're using VGG19 as a fixed feature extractor, not training it\n",
    "- Ensures consistent outputs every time\n",
    "\n",
    "### What This Achieves:\n",
    "We now have a powerful \"feature detector\" that can look at any image and tell us:\n",
    "- What low-level patterns it contains (early layers)\n",
    "- What high-level objects it contains (deep layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78f85458",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (1): ReLU(inplace=True)\n",
       "  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (3): ReLU(inplace=True)\n",
       "  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (6): ReLU(inplace=True)\n",
       "  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (8): ReLU(inplace=True)\n",
       "  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (11): ReLU(inplace=True)\n",
       "  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (13): ReLU(inplace=True)\n",
       "  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (15): ReLU(inplace=True)\n",
       "  (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (17): ReLU(inplace=True)\n",
       "  (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (20): ReLU(inplace=True)\n",
       "  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (22): ReLU(inplace=True)\n",
       "  (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (24): ReLU(inplace=True)\n",
       "  (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (26): ReLU(inplace=True)\n",
       "  (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (29): ReLU(inplace=True)\n",
       "  (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (31): ReLU(inplace=True)\n",
       "  (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (33): ReLU(inplace=True)\n",
       "  (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (35): ReLU(inplace=True)\n",
       "  (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg_19=models.vgg19(pretrained=True).features.eval()\n",
    "vgg_19"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffe5d40",
   "metadata": {},
   "source": [
    "## Step 2: Create Modified VGG Model for Feature Extraction\n",
    "\n",
    "### What We Did:\n",
    "Created a custom wrapper that extracts features from specific intermediate layers instead of just the final output.\n",
    "\n",
    "### Why We Need This:\n",
    "- Default VGG19 only returns the final output\n",
    "- We need features from MULTIPLE layers (early, middle, deep)\n",
    "- Different layers capture different aspects of the image\n",
    "\n",
    "### The Code Explained:\n",
    "\n",
    "```python\n",
    "class ModifiedVGG(nn.Module):\n",
    "```\n",
    "**Why inherit from `nn.Module`?** Makes our class a proper PyTorch model with forward propagation capabilities.\n",
    "\n",
    "```python\n",
    "def __init__(self):\n",
    "    super(ModifiedVGG, self).__init__()\n",
    "```\n",
    "**Why `super()`?** Initializes the parent `nn.Module` class, giving us access to PyTorch's model functionality.\n",
    "\n",
    "```python\n",
    "self.chosen_features = ['0', '5', '10', '19', '28']\n",
    "```\n",
    "**These specific layers were chosen because:**\n",
    "- **Layer 0**: `conv1_1` - First convolutional layer\n",
    "  - Captures basic edges, colors (very low-level)\n",
    "  - **Why?** Style includes color schemes and basic brush strokes\n",
    "  \n",
    "- **Layer 5**: `conv2_1` - After first max pooling\n",
    "  - Captures simple textures (2x downsampled)\n",
    "  - **Why?** Artistic styles have characteristic texture patterns\n",
    "  \n",
    "- **Layer 10**: `conv3_1` - After second max pooling\n",
    "  - Captures more complex textures (4x downsampled)\n",
    "  - **Why?** Combines simple textures into complex patterns\n",
    "  \n",
    "- **Layer 19**: `conv4_1` - After third max pooling\n",
    "  - Captures object parts and structural elements (8x downsampled)\n",
    "  - **Why?** Balances style and content information\n",
    "  \n",
    "- **Layer 28**: `conv5_1` - After fourth max pooling\n",
    "  - Captures high-level content (16x downsampled)\n",
    "  - **Why?** Represents WHAT is in the image (the content we want to preserve)\n",
    "\n",
    "```python\n",
    "self.model = vgg_19[:29]\n",
    "```\n",
    "**Why `[:29]`?** Takes layers 0 through 28 (up to conv5_1), discarding the rest.\n",
    "\n",
    "```python\n",
    "def forward(self, x):\n",
    "    features = []\n",
    "```\n",
    "**The forward pass** - what happens when we pass an image through the model.\n",
    "\n",
    "```python\n",
    "for layer_number, layer in enumerate(self.model):\n",
    "    x = layer(x)\n",
    "```\n",
    "**Step-by-step processing:**\n",
    "- `enumerate()` gives us both the layer number and the layer itself\n",
    "- `x = layer(x)` passes the image through one layer at a time\n",
    "- `x` gets updated with the output of each layer\n",
    "\n",
    "```python\n",
    "if str(layer_number) in self.chosen_features:\n",
    "    features.append(x)\n",
    "```\n",
    "**Selective feature extraction:**\n",
    "- When we hit one of our chosen layers (0, 5, 10, 19, 28)\n",
    "- Save that layer's output (feature map) to our list\n",
    "- Convert `layer_number` to string to match our chosen_features list\n",
    "\n",
    "```python\n",
    "return features\n",
    "```\n",
    "**Returns a list of 5 feature maps** - one from each chosen layer.\n",
    "\n",
    "### What This Achieves:\n",
    "We can now pass any image and get 5 feature maps representing:\n",
    "1. Basic visual elements (edges, colors)\n",
    "2-4. Increasingly complex textures and patterns\n",
    "5. High-level semantic content (what objects are present)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "480fb956",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModifiedVGG(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ModifiedVGG,self).__init__()\n",
    "        \n",
    "        self.chosen_features = ['0','5','10','19','28']\n",
    "        self.model = vgg_19[:29]\n",
    "    \n",
    "    def forward(self,x):\n",
    "        features = []\n",
    "        for layer_number, layer in enumerate(self.model):\n",
    "            x = layer(x)\n",
    "            if str(layer_number) in self.chosen_features:\n",
    "                features.append(x)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf7360d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModifiedVGG()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc412e5d",
   "metadata": {},
   "source": [
    "## Step 3: Image Loading and Preprocessing\n",
    "\n",
    "### What We Did:\n",
    "Created a pipeline to load images and prepare them for the neural network.\n",
    "\n",
    "### Why Preprocessing is Critical:\n",
    "Neural networks are picky - they need:\n",
    "- Specific input dimensions\n",
    "- Normalized pixel values\n",
    "- Proper tensor format\n",
    "- Correct batch structure\n",
    "\n",
    "### The Code Explained:\n",
    "\n",
    "```python\n",
    "loader = transforms.Compose([...])\n",
    "```\n",
    "**`transforms.Compose`**: Chains multiple transformations together\n",
    "- **Why?** Apply multiple preprocessing steps in sequence\n",
    "- Like a pipeline: image → resize → tensorize → output\n",
    "\n",
    "```python\n",
    "transforms.Resize((224, 224))\n",
    "```\n",
    "**Resizes image to 224×224 pixels**\n",
    "- **Why 224×224?** VGG19 was trained on this size\n",
    "- All images must be the same size for batch processing\n",
    "- **What happens?** Image is stretched/squeezed to fit\n",
    "- **Trade-off**: May distort aspect ratio, but necessary for VGG\n",
    "\n",
    "```python\n",
    "transforms.ToTensor()\n",
    "```\n",
    "**Converts PIL Image to PyTorch Tensor**\n",
    "- **Before**: PIL Image with pixels as integers [0, 255]\n",
    "- **After**: Tensor with values as floats [0.0, 1.0]\n",
    "- **Shape change**: (H, W, C) → (C, H, W)\n",
    "  - Moves channels first: Height, Width, Channels → Channels, Height, Width\n",
    "- **Why?** PyTorch expects channel-first format for convolutions\n",
    "\n",
    "```python\n",
    "def load_image(image_path):\n",
    "```\n",
    "**Function to load and preprocess a single image**\n",
    "\n",
    "```python\n",
    "image = Image.open(image_path).convert('RGB')\n",
    "```\n",
    "**Opens image file and ensures RGB format**\n",
    "- **Why `convert('RGB')`?** Some images are RGBA (with alpha channel) or grayscale\n",
    "- RGB ensures 3 channels: Red, Green, Blue\n",
    "- VGG19 expects exactly 3 input channels\n",
    "\n",
    "```python\n",
    "image = loader(image).unsqueeze(0)\n",
    "```\n",
    "**Two operations here:**\n",
    "\n",
    "1. **`loader(image)`**: Applies our preprocessing pipeline\n",
    "   - After this: shape is [3, 224, 224] (channels, height, width)\n",
    "\n",
    "2. **`.unsqueeze(0)`**: Adds a batch dimension at position 0\n",
    "   - Before: [3, 224, 224]\n",
    "   - After: [1, 3, 224, 224] (batch_size=1, channels, height, width)\n",
    "   - **Why?** PyTorch models expect batches of images, even if batch size is 1\n",
    "   - The model processes in format: [N, C, H, W] where N = number of images\n",
    "\n",
    "```python\n",
    "return image\n",
    "```\n",
    "**Returns a tensor ready to feed into VGG19**\n",
    "\n",
    "### What This Achieves:\n",
    "- Loads any image file\n",
    "- Standardizes to VGG19's expected format\n",
    "- Returns a tensor with shape [1, 3, 224, 224]\n",
    "- Ready for feature extraction!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b2459ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loader=transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "def load_image(image_path):\n",
    "    image=Image.open(image_path).convert('RGB')\n",
    "    image=loader(image).unsqueeze(0)\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5e82efbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_image=load_image('images/test/anna_hathaway.jpg')\n",
    "style_image=load_image('images/styles/acrylic_style.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8957cd35",
   "metadata": {},
   "source": [
    "## Step 4: Load Content and Style Images\n",
    "\n",
    "### What We Did:\n",
    "```python\n",
    "original_image = load_image('images/test/anna_hathaway.jpg')\n",
    "style_image = load_image('images/styles/acrylic_style.jpg')\n",
    "```\n",
    "\n",
    "### Why These Two Images:\n",
    "- **Content Image (original_image)**: The photo we want to preserve the structure of\n",
    "  - Contains the \"what\" - the objects, composition, layout\n",
    "  - Example: A photo of a person, landscape, building\n",
    "  \n",
    "- **Style Image**: The artwork whose style we want to mimic\n",
    "  - Contains the \"how\" - the textures, colors, brush strokes, artistic patterns\n",
    "  - Example: A Van Gogh painting, impressionist artwork\n",
    "\n",
    "### What Happens:\n",
    "Both images are now tensors of shape [1, 3, 224, 224], ready to be analyzed by VGG19.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 5: Initialize the Generated Image\n",
    "\n",
    "### What We Did:\n",
    "```python\n",
    "generated = original_image.clone().requires_grad_(True)\n",
    "```\n",
    "\n",
    "### The Revolutionary Concept Here:\n",
    "\n",
    "In normal deep learning:\n",
    "- **Input**: Fixed (image)\n",
    "- **Model weights**: Optimized (trained)\n",
    "- **Output**: Predicted labels\n",
    "\n",
    "In style transfer:\n",
    "- **Input**: Optimized (the generated image) ← We change THIS\n",
    "- **Model weights**: Fixed (pre-trained VGG19) ← This stays constant\n",
    "- **Output**: Used to compute loss\n",
    "\n",
    "### Line-by-Line Breakdown:\n",
    "\n",
    "```python\n",
    "original_image.clone()\n",
    "```\n",
    "**Creates a copy of the content image**\n",
    "- **Why start with content image?** Gives us a head start\n",
    "- We already have the right content, just need to adjust the style\n",
    "- **Alternative**: Could start with random noise, but convergence is slower\n",
    "\n",
    "```python\n",
    ".requires_grad_(True)\n",
    "```\n",
    "**This is the KEY line - enables pixel optimization**\n",
    "- **What it does**: Tells PyTorch to track all operations on this tensor\n",
    "- **Why?** So we can compute gradients with respect to pixel values\n",
    "- **Effect**: We can do backpropagation to find how to change each pixel\n",
    "\n",
    "**Normally**: Gradients flow back to update model weights  \n",
    "**Here**: Gradients flow back to update pixel values\n",
    "\n",
    "### The Optimization Strategy:\n",
    "1. Start with content image pixels\n",
    "2. Calculate how different it is from our targets (content + style)\n",
    "3. Compute gradient: \"How should we change each pixel to reduce the loss?\"\n",
    "4. Update pixels using these gradients\n",
    "5. Repeat until we match both content and style\n",
    "\n",
    "### What This Achieves:\n",
    "We have a mutable image that we can optimize, pixel by pixel, to achieve our artistic goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97eca336",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated=original_image.clone().requires_grad_(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6364b64f",
   "metadata": {},
   "source": [
    "## Step 6: Set Hyperparameters\n",
    "\n",
    "### What We Did:\n",
    "```python\n",
    "total_steps = 2000\n",
    "learning_rate = 0.0003\n",
    "alpha = 1\n",
    "beta = 0.01\n",
    "```\n",
    "\n",
    "### Why Each Parameter Matters:\n",
    "\n",
    "```python\n",
    "total_steps = 2000\n",
    "```\n",
    "**Number of optimization iterations**\n",
    "- **Typical range**: 1000-6000 steps depending on complexity\n",
    "\n",
    "```python\n",
    "learning_rate = 0.0003\n",
    "```\n",
    "**How much we change pixels in each step**\n",
    "- **What it means**: Each pixel can change by at most 0.03% of its range per step\n",
    "- **This value (0.0003)**: Proven sweet spot for image optimization\n",
    "\n",
    "```python\n",
    "alpha = 1\n",
    "```\n",
    "**Weight for content loss**\n",
    "- **What it controls**: How much we care about preserving the original content\n",
    "- **Value of 1**: Content loss contributes fully to total loss\n",
    "- **Higher alpha**: Generated image looks more like original photo\n",
    "- **Lower alpha**: Generated image can deviate more from original structure\n",
    "\n",
    "```python\n",
    "beta = 0.01\n",
    "```\n",
    "**Weight for style loss**\n",
    "- **What it controls**: How much we care about matching the artistic style\n",
    "- **Value of 0.01**: Style loss is 1/100th as important as content loss\n",
    "- **Higher beta**: Stronger style transfer, but may lose content structure\n",
    "- **Lower beta**: Weaker style transfer, keeps more photorealistic look\n",
    "\n",
    "### The Critical Balance (alpha/beta ratio):\n",
    "\n",
    "**Current ratio**: alpha/beta = 1/0.01 = 100\n",
    "- This means: \"Content is 100× more important than style\"\n",
    "- **Result**: You'll clearly see the original subject, painted in the style\n",
    "\n",
    "**Different ratios produce different effects:**\n",
    "- **alpha=1, beta=0.001** (ratio 1000): Subtle style hints, mostly original photo\n",
    "- **alpha=1, beta=0.1** (ratio 10): Strong stylization, may lose some content clarity\n",
    "- **alpha=1, beta=1** (ratio 1): Heavy stylization, abstract result\n",
    "\n",
    "### Why This Specific Combination:\n",
    "- **2000 steps**: Enough for convergence without wasting time\n",
    "- **lr=0.0003**: Stable, proven rate for pixel optimization\n",
    "- **alpha=1, beta=0.01**: Produces aesthetically pleasing results where you can clearly recognize the content image but see the artistic style applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "84beba17",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_steps=2000\n",
    "learning_rate=0.0003\n",
    "alpha = 1\n",
    "beta = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2e5c3d",
   "metadata": {},
   "source": [
    "## Step 7: The Training Loop - Where the Magic Happens\n",
    "\n",
    "### Overview of What We're Doing:\n",
    "This is an **iterative optimization process** where we repeatedly:\n",
    "1. Extract features from our current generated image\n",
    "2. Compare them to content and style targets\n",
    "3. Calculate how \"wrong\" we are (loss)\n",
    "4. Update pixels to be more \"right\"\n",
    "5. Repeat until convergence\n",
    "\n",
    "---\n",
    "\n",
    "## Part 1: Setup\n",
    "\n",
    "```python\n",
    "from torchvision.utils import save_image\n",
    "```\n",
    "**Utility to save tensor as image file** - we'll use this to save progress\n",
    "\n",
    "```python\n",
    "optimizer = optim.Adam([generated], lr=learning_rate)\n",
    "```\n",
    "**Creates an Adam optimizer to update pixel values**\n",
    "\n",
    "**What is Adam?**\n",
    "- Adaptive Moment Estimation - advanced gradient descent algorithm\n",
    "- Automatically adjusts learning rate for each parameter (pixel)\n",
    "- Better than basic SGD for this task\n",
    "\n",
    "**`[generated]`**: List of tensors to optimize\n",
    "- **Critical**: We're optimizing the IMAGE, not model weights!\n",
    "- This tensor's values (pixels) will be updated each iteration\n",
    "\n",
    "**`lr=learning_rate`**: How big our update steps are (0.0003)\n",
    "\n",
    "---\n",
    "\n",
    "## Part 2: Main Loop\n",
    "\n",
    "```python\n",
    "for step in tqdm(range(total_steps), desc=\"Training\"):\n",
    "```\n",
    "**Iterates 2000 times** with a progress bar (tqdm shows status)\n",
    "\n",
    "---\n",
    "\n",
    "## Part 3: Feature Extraction\n",
    "\n",
    "```python\n",
    "generated_features = model(generated)\n",
    "original_image_features = model(original_image)\n",
    "style_image_features = model(style_image)\n",
    "```\n",
    "\n",
    "**What happens here:**\n",
    "Each line passes an image through our ModifiedVGG and gets 5 feature maps (from layers 0, 5, 10, 19, 28).\n",
    "\n",
    "**`model(generated)`**: \n",
    "- Input: Current generated image [1, 3, 224, 224]\n",
    "- Output: List of 5 tensors, each a feature map\n",
    "- **Why?** Need to see what the current generated image looks like to VGG\n",
    "\n",
    "**`model(original_image)`**: \n",
    "- Content target - these features should match generated's features\n",
    "- **Why?** To preserve the content (objects, structure)\n",
    "\n",
    "**`model(style_image)`**: \n",
    "- Style target - the Gram matrices should match\n",
    "- **Why?** To transfer the artistic style\n",
    "\n",
    "**Key insight**: We extract features from ALL THREE images to compare them.\n",
    "\n",
    "---\n",
    "\n",
    "## Part 4: Initialize Losses\n",
    "\n",
    "```python\n",
    "style_loss = 0\n",
    "original_loss = 0\n",
    "```\n",
    "**Accumulators for loss values across all layers**\n",
    "- Start at 0, add contribution from each layer\n",
    "- By the end, they'll represent total content and style differences\n",
    "\n",
    "---\n",
    "\n",
    "## Part 5: Layer-by-Layer Loss Computation\n",
    "\n",
    "```python\n",
    "for gen_feature, orig_feature, style_feature in zip(generated_features, original_image_features, style_image_features):\n",
    "```\n",
    "\n",
    "**Iterates through corresponding feature maps from all three images**\n",
    "- **Iteration 1**: Layer 0 features from all three images\n",
    "- **Iteration 2**: Layer 5 features from all three images\n",
    "- ... (5 iterations total)\n",
    "\n",
    "**`zip()`**: Groups corresponding items from three lists together\n",
    "\n",
    "---\n",
    "\n",
    "## Part 6: Content Loss\n",
    "\n",
    "```python\n",
    "batch_size, channel, height, width = gen_feature.shape\n",
    "```\n",
    "**Unpacks the dimensions of the feature map**\n",
    "- `batch_size = 1` (we're processing one image)\n",
    "- `channel = 64` (layer 0) up to `512` (layer 28) - number of filters\n",
    "- `height, width = 224, 224` (gets smaller in deeper layers due to pooling)\n",
    "\n",
    "**Why extract these?** We'll need them for reshaping in Gram matrix calculation.\n",
    "\n",
    "```python\n",
    "original_loss += torch.mean((gen_feature - orig_feature)**2)\n",
    "```\n",
    "\n",
    "**This is the content loss calculation - let's break it down:**\n",
    "\n",
    "**Mathematical formula**: \n",
    "$$L_{content} = \\frac{1}{N} \\sum_{i=1}^{N} (F_{gen}^i - F_{orig}^i)^2$$\n",
    "\n",
    "**`(gen_feature - orig_feature)`**: \n",
    "- Element-wise subtraction of feature maps\n",
    "- If generated matches original perfectly, this is 0\n",
    "- Larger values = bigger difference\n",
    "\n",
    "**`**2`**: \n",
    "- Squares the differences\n",
    "- Makes all values positive\n",
    "- Penalizes large differences more heavily (quadratic penalty)\n",
    "\n",
    "**`torch.mean()`**: \n",
    "- Averages over all positions and channels\n",
    "- Gives us one number representing \"how different\" the features are\n",
    "\n",
    "**`+=`**: \n",
    "- Adds this layer's contribution to total content loss\n",
    "- We sum across all 5 layers for complete content representation\n",
    "\n",
    "**Why this works**: \n",
    "- If generated has same features as content image → loss is low\n",
    "- Features in deep layers represent semantic content (what objects exist)\n",
    "- Matching features = matching content\n",
    "\n",
    "---\n",
    "\n",
    "## Part 7: Style Loss (The Gram Matrix)\n",
    "\n",
    "### What is a Gram Matrix and Why Do We Need It?\n",
    "\n",
    "**The Problem**: \n",
    "- Feature maps contain spatial information (where things are)\n",
    "- Style shouldn't care about WHERE features occur, only HOW they co-occur\n",
    "- A painting's style is the same whether the tree is on the left or right\n",
    "\n",
    "**The Solution - Gram Matrix**:\n",
    "- Measures correlations between different feature channels\n",
    "- \"Do vertical edges occur together with red color?\"\n",
    "- \"Do curved lines appear with blue tones?\"\n",
    "- **Removes spatial information**, keeps only feature relationships\n",
    "\n",
    "### The Mathematics:\n",
    "\n",
    "Given feature map $F$ with shape $[C, H, W]$ (channels, height, width):\n",
    "\n",
    "**Gram matrix formula**:\n",
    "$$G_{ij} = \\sum_{k=1}^{H \\times W} F_{ik} \\cdot F_{jk}$$\n",
    "\n",
    "Where:\n",
    "- $G_{ij}$ = correlation between channel $i$ and channel $j$\n",
    "- $F_{ik}$ = activation of channel $i$ at spatial position $k$\n",
    "- Sum over all spatial positions (H×W locations)\n",
    "\n",
    "**What this means**:\n",
    "- If channels $i$ and $j$ are both highly activated together → large $G_{ij}$\n",
    "- If they never activate together → small $G_{ij}$\n",
    "- **Result**: A $C \\times C$ matrix encoding style without spatial info\n",
    "\n",
    "---\n",
    "\n",
    "### The Code Implementation:\n",
    "\n",
    "```python\n",
    "G = gen_feature.view(channel, height*width).mm(\n",
    "    gen_feature.view(channel, height*width).t()\n",
    ")\n",
    "```\n",
    "\n",
    "**Let's break this down step by step:**\n",
    "\n",
    "**Step 1: `gen_feature.view(channel, height*width)`**\n",
    "- **Before**: shape [1, C, H, W] e.g., [1, 64, 224, 224]\n",
    "- **After**: shape [C, H×W] e.g., [64, 50176]\n",
    "- **What it does**: Flattens spatial dimensions into one dimension\n",
    "- **Why?** To treat each spatial position as a data point\n",
    "\n",
    "**Step 2: `.mm()`** \n",
    "- Matrix multiplication\n",
    "- **PyTorch function**: `torch.mm(A, B)` computes A @ B\n",
    "\n",
    "**Step 3: `.t()`**\n",
    "- Transpose operation\n",
    "- **Effect**: Swaps rows and columns\n",
    "- If input is [C, H×W], output is [H×W, C]\n",
    "\n",
    "**The complete operation**:\n",
    "```\n",
    "[C, H×W] @ [H×W, C] = [C, C]\n",
    "```\n",
    "\n",
    "**Example with actual numbers** (layer 0: 64 channels, 224×224 spatial):\n",
    "```\n",
    "[64, 50176] @ [50176, 64] = [64, 64]\n",
    "```\n",
    "\n",
    "**What each element represents**:\n",
    "- G[i, j] = dot product of channel i with channel j across all spatial positions\n",
    "- High value → channels i and j activate together (style correlation)\n",
    "- This is EXACTLY the Gram matrix formula!\n",
    "\n",
    "```python\n",
    "A = style_feature.view(channel, height*width).mm(\n",
    "    style_feature.view(channel, height*width).t()\n",
    ")\n",
    "```\n",
    "**Same calculation for the style image**\n",
    "- **G**: Gram matrix of current generated image\n",
    "- **A**: Gram matrix of target style image\n",
    "\n",
    "---\n",
    "\n",
    "### Computing Style Loss:\n",
    "\n",
    "```python\n",
    "style_loss += torch.mean((G - A)**2)\n",
    "```\n",
    "\n",
    "**Mathematical formula**:\n",
    "$$L_{style} = \\frac{1}{C^2} \\sum_{i=1}^{C} \\sum_{j=1}^{C} (G_{ij} - A_{ij})^2$$\n",
    "\n",
    "**What happens**:\n",
    "- **`(G - A)`**: Difference between generated and style Gram matrices\n",
    "  - If style matches perfectly → difference is 0\n",
    "- **`**2`**: Square the differences (MSE loss)\n",
    "- **`torch.mean()`**: Average over all $C \\times C$ elements\n",
    "- **`+=`**: Add contribution from this layer to total style loss\n",
    "\n",
    "**Why this works**:\n",
    "- Gram matrix captures style (texture correlations)\n",
    "- Matching Gram matrices = matching style\n",
    "- Independent of spatial arrangement = style transfer without copying positions\n",
    "\n",
    "---\n",
    "\n",
    "## Part 8: Total Loss and Optimization\n",
    "\n",
    "```python\n",
    "total_loss = alpha*original_loss + beta*style_loss\n",
    "```\n",
    "\n",
    "**Combines both losses with our chosen weights**\n",
    "\n",
    "**Mathematical formula**:\n",
    "$$L_{total} = \\alpha \\cdot L_{content} + \\beta \\cdot L_{style}$$\n",
    "\n",
    "With $\\alpha = 1$ and $\\beta = 0.01$:\n",
    "$$L_{total} = 1 \\cdot L_{content} + 0.01 \\cdot L_{style}$$\n",
    "\n",
    "**What this means**:\n",
    "- Content is 100× more important than style\n",
    "- If content loss = 10 and style loss = 100\n",
    "- Total loss = 1×10 + 0.01×100 = 10 + 1 = 11\n",
    "- Mostly driven by content preservation\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "optimizer.zero_grad()\n",
    "```\n",
    "**Clears previous gradients**\n",
    "- **Why?** PyTorch accumulates gradients by default\n",
    "- Without this, gradients from previous iterations would add up\n",
    "- We want fresh gradients for each iteration\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "total_loss.backward()\n",
    "```\n",
    "**THE MOST IMPORTANT LINE - Computes gradients**\n",
    "\n",
    "**What happens internally**:\n",
    "1. PyTorch traces back through all operations that created `total_loss`\n",
    "2. Computes $\\frac{\\partial L_{total}}{\\partial pixel_{i,j}}$ for EVERY pixel\n",
    "3. These gradients tell us: \"Increase or decrease this pixel to reduce loss?\"\n",
    "4. Stored in `generated.grad`\n",
    "\n",
    "**The chain rule in action**:\n",
    "$$\\frac{\\partial L_{total}}{\\partial pixel} = \\frac{\\partial L_{total}}{\\partial features} \\cdot \\frac{\\partial features}{\\partial pixel}$$\n",
    "\n",
    "**Key insight**: \n",
    "- Gradients flow backwards through VGG19 (which is frozen)\n",
    "- End up in the input pixels (which are trainable)\n",
    "- We now know how to adjust each pixel to minimize loss!\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "optimizer.step()\n",
    "```\n",
    "**Updates the pixel values using gradients**\n",
    "\n",
    "**Adam update rule** (simplified):\n",
    "$$pixel_{new} = pixel_{old} - learning\\_rate \\cdot gradient$$\n",
    "\n",
    "**What happens**:\n",
    "- For each pixel, Adam computes an update based on gradient\n",
    "- Applies learning rate (0.0003) to control step size\n",
    "- Adjusts pixels in the direction that reduces loss\n",
    "- `generated` tensor now has updated pixel values\n",
    "\n",
    "**Result**: Our image is now slightly more stylized and still maintains content!\n",
    "\n",
    "---\n",
    "\n",
    "## Part 9: Logging Progress\n",
    "\n",
    "```python\n",
    "if step % 200 == 0:\n",
    "    tqdm.write(\"Total loss at step {}: {}\".format(step, total_loss.item()))\n",
    "    save_image(generated, \"results/generated.png\")\n",
    "```\n",
    "\n",
    "**Every 200 iterations**:\n",
    "- **`step % 200 == 0`**: True when step is 0, 200, 400, ..., 1800\n",
    "- **`tqdm.write()`**: Prints without breaking progress bar\n",
    "- **`total_loss.item()`**: Converts tensor to Python number\n",
    "- **`save_image()`**: Saves current generated image to file\n",
    "\n",
    "**Why log every 200 steps?**\n",
    "- Can monitor if loss is decreasing (optimization working)\n",
    "- Can see intermediate results\n",
    "- Not every step (too much output), not too rare (want feedback)\n",
    "\n",
    "---\n",
    "\n",
    "## The Complete Flow:\n",
    "\n",
    "1. **Extract features** from generated, content, and style images\n",
    "2. **For each layer**:\n",
    "   - Compare generated vs content features → **content loss**\n",
    "   - Compute Gram matrices for generated and style → **style loss**\n",
    "3. **Combine losses** with weights α and β → **total loss**\n",
    "4. **Compute gradients**: How should each pixel change?\n",
    "5. **Update pixels**: Apply gradients with Adam optimizer\n",
    "6. **Repeat** for 2000 iterations\n",
    "7. **Result**: An image that looks like the content image painted in the style image's artistic style!\n",
    "\n",
    "---\n",
    "\n",
    "## The Mathematical Beauty:\n",
    "\n",
    "By optimizing:\n",
    "$$\\min_{generated} \\left[ \\alpha \\sum_{l} \\|F^l_{generated} - F^l_{content}\\|^2 + \\beta \\sum_{l} \\|G^l_{generated} - G^l_{style}\\|^2 \\right]$$\n",
    "\n",
    "We find the image that simultaneously:\n",
    "- Has the same **semantic content** (objects, structure) as the original photo\n",
    "- Has the same **artistic style** (textures, patterns, colors) as the style painting\n",
    "\n",
    "This is neural style transfer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5effaf20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import save_image\n",
    "\n",
    "optimizer=optim.Adam([generated],lr=learning_rate)\n",
    "\n",
    "for step in tqdm(range(total_steps), desc=\"Training\"):\n",
    "    generated_features=model(generated)\n",
    "    original_image_features=model(original_image)\n",
    "    style_image_features=model(style_image)\n",
    "    \n",
    "    style_loss=0\n",
    "    original_loss=0\n",
    "    \n",
    "    for gen_feature, orig_feature, style_feature in zip(generated_features, original_image_features, style_image_features):\n",
    "        \n",
    "        batch_size,channel,height,width=gen_feature.shape\n",
    "        original_loss+=torch.mean((gen_feature - orig_feature)**2)\n",
    "        \n",
    "        #compute the gram matrix\n",
    "        G=gen_feature.view(channel, height*width).mm(gen_feature.view(channel, height*width).t()) # mm = matrix multiplication\n",
    "        A=style_feature.view(channel, height*width).mm(style_feature.view(channel, height*width).t())\n",
    "        \n",
    "        style_loss+=torch.mean((G - A)**2) \n",
    "    total_loss= alpha*original_loss + beta*style_loss\n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if step % 200 ==0:\n",
    "        tqdm.write(\"Total loss at step {}: {}\".format(step, total_loss.item()))\n",
    "        save_image(generated,\"results/generated.png\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
