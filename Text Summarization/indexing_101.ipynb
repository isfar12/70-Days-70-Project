{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b20e76af",
   "metadata": {},
   "source": [
    "# ðŸš€ Complete Tensor Indexing & Slicing Guide for Deep Learning\n",
    "\n",
    "**From Python Lists â†’ NumPy â†’ PyTorch across ALL Architectures**\n",
    "\n",
    "Master indexing patterns used in:\n",
    "- **ANN** (Dense networks): `(batch, features)`\n",
    "- **CNN** (Images): `(batch, channels, height, width)`\n",
    "- **RNN/LSTM/GRU** (Sequences): `(batch, time, hidden)`\n",
    "- **Seq2Seq** (Encoder-Decoder): `(batch, seq_len, vocab)`\n",
    "- **Transformer** (Attention): `(batch, heads, seq, seq)`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2988a064",
   "metadata": {},
   "source": [
    "## SECTION 0: Concept Overview\n",
    "\n",
    "### Why Learn Indexing?\n",
    "- **Training:** Load batches, compute losses, backpropagate gradients\n",
    "- **Inference:** Extract predictions, decode sequences token-by-token\n",
    "- **Debugging:** Inspect individual samples and their representations\n",
    "\n",
    "### Three Mental Models (Stack them!)\n",
    "1. **Lists** (Python): `x[start:stop:step]` - basic slicing\n",
    "2. **Arrays** (NumPy): extend slicing to N dimensions\n",
    "3. **Tensors** (PyTorch): same as NumPy + GPU + automatic differentiation\n",
    "\n",
    "### Shape Notation Key\n",
    "```\n",
    "(batch, time, features)      â† 3D tensor\n",
    "  â†“      â†“      â†“\n",
    "batch: how many samples process in parallel\n",
    "time:  how many timesteps/positions in sequence\n",
    "features: dimension per position (hidden size, vocab size, etc.)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c2f040",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## SECTION 1: Python Lists (Foundation)\n",
    "\n",
    "### What: Basic Slicing Syntax\n",
    "**`list[start:stop:step]`**\n",
    "- `start`: begin index (inclusive, default=0)\n",
    "- `stop`: end index (EXCLUSIVE, default=len)\n",
    "- `step`: interval (default=1)\n",
    "- Negative indices count from end: `-1` = last element\n",
    "\n",
    "### Why: Every indexing operation you'll see traces back to this\n",
    "\n",
    "### Where: Used in\n",
    "- Batch extraction: `data[0:32]` get first 32 samples\n",
    "- Sequence manipulation: `seq[:-1]` remove last token\n",
    "- Data augmentation: `seq[::-1]` reverse sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ecb82085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic Slicing:\n",
      "Original list: lst = [10, 11, 12, 13, 14]\n",
      "lst[0]       = 10           # first element\n",
      "lst[-1]      = 14          # last element\n",
      "lst[1:4]     = [11, 12, 13]         # elements at indices 1,2,3 (not 4!)\n",
      "lst[:3]      = [10, 11, 12]          # first 3 elements\n",
      "lst[2:]      = [12, 13, 14]          # from index 2 to end\n",
      "lst[::2]     = [10, 12, 14]         # every 2nd element\n",
      "lst[::-1]    = [14, 13, 12, 11, 10]        # reversed\n",
      "\n",
      "Real-world examples in Deep Learning:\n",
      "- Training loop: data[i*batch_size:(i+1)*batch_size]  # extract batch\n",
      "- Teacher forcing: inputs = seq[:-1], targets = seq[1:]  # shift for supervision\n",
      "- Validation: val_indices = range(len(data))[train_size:]  # holdout split\n"
     ]
    }
   ],
   "source": [
    "# Python List Slicing - Foundation of Everything\n",
    "lst = [10, 11, 12, 13, 14]\n",
    "\n",
    "print(\"Basic Slicing:\")\n",
    "print(f\"Original list: lst = {lst}\")\n",
    "print(f\"lst[0]       = {lst[0]}           # first element\")\n",
    "print(f\"lst[-1]      = {lst[-1]}          # last element\")\n",
    "print(f\"lst[1:4]     = {lst[1:4]}         # elements at indices 1,2,3 (not 4!)\")\n",
    "print(f\"lst[:3]      = {lst[:3]}          # first 3 elements\")\n",
    "print(f\"lst[2:]      = {lst[2:]}          # from index 2 to end\")\n",
    "print(f\"lst[::2]     = {lst[::2]}         # every 2nd element\")\n",
    "print(f\"lst[::-1]    = {lst[::-1]}        # reversed\")\n",
    "print()\n",
    "\n",
    "print(\"Real-world examples in Deep Learning:\")\n",
    "print(\"- Training loop: data[i*batch_size:(i+1)*batch_size]  # extract batch\")\n",
    "print(\"- Teacher forcing: inputs = seq[:-1], targets = seq[1:]  # shift for supervision\")\n",
    "print(\"- Validation: val_indices = range(len(data))[train_size:]  # holdout split\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4881fedd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## SECTION 2: NumPy Arrays (Extend to Multiple Dimensions)\n",
    "\n",
    "### What: Same slicing rules but for N-dimensional arrays\n",
    "Shape notation: `(dim0, dim1, dim2, ...)`\n",
    "\n",
    "### Why: \n",
    "- Preprocess data (images as arrays, sequences as arrays)\n",
    "- Load batches efficiently\n",
    "- Extract features before feeding to neural networks\n",
    "\n",
    "### Where: Used in\n",
    "- Image loading: `image[100:150, 50:100, 0]` crop + select channel\n",
    "- Batch preparation: `X[train_indices, :]` get training samples\n",
    "- Feature extraction: `features[:, important_features]` select columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "763e2571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array shape (2, 3, 4):\n",
      "[[[ 0  1  2  3]\n",
      "  [ 4  5  6  7]\n",
      "  [ 8  9 10 11]]\n",
      "\n",
      " [[12 13 14 15]\n",
      "  [16 17 18 19]\n",
      "  [20 21 22 23]]]\n",
      "\n",
      "Indexing Examples:\n",
      "A[0].shape        = (3, 4)      # first batch (3, 4)\n",
      "Array A[0]:\n",
      "[[ 0  1  2  3]\n",
      " [ 4  5  6  7]\n",
      " [ 8  9 10 11]]\n",
      "\n",
      "A[0, 1].shape     = (4,)     # batch 0, row 1: (4,)\n",
      "A[0, 1, 2]        = 6        # batch 0, row 1, col 2: scalar\n",
      "A[:, 1, :].shape  = (2, 4)  # all batches, row 1: (2, 4)\n",
      "A[:, 1, :]:\n",
      "[[ 4  5  6  7]\n",
      " [16 17 18 19]]\n",
      "\n",
      "A[:, :, -1].shape = (2, 3) # all batches, last column: (2, 3)\n",
      "A[:, :, -1]:\n",
      "[[ 3  7 11]\n",
      " [15 19 23]]\n",
      "\n",
      "A[..., -1].shape  = (2, 3)  # same as above using ellipsis\n",
      "A[..., -1]:\n",
      "[[ 3  7 11]\n",
      " [15 19 23]]\n",
      "\n",
      "\n",
      "Range slicing:\n",
      "A[:, :2, :2].shape = (2, 2, 2)  # first 2x2 patch: (2, 2, 2)\n",
      "\n",
      "Boolean masking:\n",
      "Even elements count: 12\n",
      "First 5 even values: [0 2 4 6 8]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create 3D array: (batch=2, height=3, width=4)\n",
    "A = np.arange(2*3*4).reshape(2, 3, 4)\n",
    "print(\"Array shape (2, 3, 4):\")\n",
    "print(A)\n",
    "print()\n",
    "\n",
    "print(\"Indexing Examples:\")\n",
    "print(f\"A[0].shape        = {A[0].shape}      # first batch (3, 4)\")\n",
    "print(f\"Array A[0]:\\n{A[0]}\\n\")\n",
    "print(f\"A[0, 1].shape     = {A[0, 1].shape}     # batch 0, row 1: (4,)\")\n",
    "print(f\"A[0, 1, 2]        = {A[0, 1, 2]}        # batch 0, row 1, col 2: scalar\")\n",
    "print(f\"A[:, 1, :].shape  = {A[:, 1, :].shape}  # all batches, row 1: (2, 4)\")\n",
    "print(f\"A[:, 1, :]:\\n{A[:, 1, :]}\\n\")\n",
    "print(f\"A[:, :, -1].shape = {A[:, :, -1].shape} # all batches, last column: (2, 3)\")\n",
    "print(f\"A[:, :, -1]:\\n{A[:, :, -1]}\\n\")\n",
    "print(f\"A[..., -1].shape  = {A[..., -1].shape}  # same as above using ellipsis\")\n",
    "print(f\"A[..., -1]:\\n{A[..., -1]}\\n\")\n",
    "print()\n",
    "\n",
    "print(\"Range slicing:\")\n",
    "print(f\"A[:, :2, :2].shape = {A[:, :2, :2].shape}  # first 2x2 patch: (2, 2, 2)\")\n",
    "print()\n",
    "print(\"Boolean masking:\")\n",
    "mask = A % 2 == 0\n",
    "print(f\"Even elements count: {mask.sum()}\")\n",
    "print(f\"First 5 even values: {A[mask][:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9865fc5f",
   "metadata": {},
   "source": [
    "### 2D Array Pattern: ANN Input\n",
    "```\n",
    "X.shape = (batch=32, features=784)\n",
    "  Row 0: [pixel1, pixel2, ..., pixel784]  â† sample 0\n",
    "  Row 1: [pixel1, pixel2, ..., pixel784]  â† sample 1\n",
    "  ...\n",
    "  Row 31: [...]\n",
    "\n",
    "Indexing: X[i, j] = sample i's feature j\n",
    "```\n",
    "\n",
    "### 3D Array Pattern: RNN/Seq2Seq\n",
    "```\n",
    "X.shape = (batch=8, seq_len=50, hidden=256)\n",
    "  [sample 0: [[t0_feat0, t0_feat1, ...],      â† t=0\n",
    "              [t1_feat0, t1_feat1, ...],      â† t=1\n",
    "              ...],\n",
    "   sample 1: [[...], [...], ...],\n",
    "   ...]\n",
    "\n",
    "Indexing: X[b, t, h] = sample b's feature at time t\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea003077",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## SECTION 3: PyTorch Tensors (GPU + Gradients)\n",
    "\n",
    "### What: Same as NumPy but with GPU support and automatic differentiation\n",
    "**`torch.Tensor`** is the standard for deep learning\n",
    "\n",
    "### Why:\n",
    "- GPU acceleration: 100x faster on NVIDIA GPUs\n",
    "- Automatic gradients: `loss.backward()` computes âˆ‡ for you\n",
    "- Deep integration with neural network libraries\n",
    "\n",
    "### Where: All your models use this\n",
    "- Forward pass: `logits = model(input)` returns PyTorch tensor\n",
    "- Loss computation: `loss = criterion(logits, targets)`\n",
    "- Backprop: gradients computed on PyTorch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f487dfaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits.shape = torch.Size([2, 5, 10])\n",
      "\n",
      "tensor([[[ 0.0761, -1.0667,  0.8995,  0.0883, -1.1413, -0.2749,  0.2849,\n",
      "           0.1230,  0.7582, -0.3187],\n",
      "         [-0.9968,  1.4641, -0.7518, -0.0999,  1.1375,  0.9930, -2.2547,\n",
      "           0.4055, -1.1677,  0.0264],\n",
      "         [-1.7402, -0.8221, -0.4678,  1.1496, -1.9136, -1.1540, -0.1769,\n",
      "           0.4162, -0.2505,  0.8826],\n",
      "         [-1.9315, -0.9520,  0.9917,  0.9765,  0.2820,  1.5582, -0.1205,\n",
      "          -0.7226, -0.5065,  0.6002],\n",
      "         [-2.1641, -0.5053, -1.1224, -1.7005, -0.1520, -0.5210, -0.1782,\n",
      "           1.1223, -0.7763, -0.3894]],\n",
      "\n",
      "        [[ 0.1289,  2.1010,  1.1848,  0.6832,  0.1190, -0.3506,  0.7557,\n",
      "          -0.8110, -1.1865, -0.0641],\n",
      "         [ 1.4803,  0.7393,  1.2105, -0.0853, -0.0061,  1.6924, -0.7216,\n",
      "           0.8889, -0.1620, -1.0572],\n",
      "         [ 1.8194,  3.4209,  0.1547,  0.1389, -1.3578, -0.6059,  1.8113,\n",
      "          -0.4743, -0.1222,  1.2391],\n",
      "         [-1.3419, -2.2105, -1.0221, -0.7443, -1.3896,  0.7736, -0.8980,\n",
      "          -0.3498,  0.9508,  0.1400],\n",
      "         [-2.4354, -0.5577, -1.9631, -0.0055, -0.4249, -0.0968,  1.0742,\n",
      "          -0.3919, -0.6684, -0.2333]]])\n",
      "\n",
      "PyTorch Indexing (same as NumPy):\n",
      "logits[0].shape       = torch.Size([5, 10])       # first batch: (5, 10)\n",
      "logits[:, 2, :].shape = torch.Size([2, 10]) # all batch, time=2: (2, 10)\n",
      "logits[:, :, 5].shape = torch.Size([2, 5]) # all batch/time, vocab=5: (2, 5)\n",
      "\n",
      "Argmax to get token predictions:\n",
      "tokens.shape = torch.Size([2, 5])  # (2, 5) - token per time step\n",
      "tokens = tensor([[2, 1, 3, 5, 7],\n",
      "        [1, 5, 1, 8, 6]])\n",
      "\n",
      "Real Seq2Seq scenario:\n",
      "targets.shape = torch.Size([2, 5])\n",
      "targets = tensor([[0, 1, 2, 3, 4],\n",
      "        [5, 6, 7, 8, 9]])\n",
      "\n",
      "After reshape for loss computation:\n",
      "logits_flat.shape  = torch.Size([10, 10])\n",
      "targets_flat.shape = torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Create tensor: (batch=2, time=5, vocab=10)\n",
    "logits = torch.randn(2, 5, 10)\n",
    "print(f\"logits.shape = {logits.shape}\")\n",
    "print()\n",
    "print(logits)\n",
    "print()\n",
    "print(\"PyTorch Indexing (same as NumPy):\")\n",
    "print(f\"logits[0].shape       = {logits[0].shape}       # first batch: (5, 10)\")\n",
    "print(f\"logits[:, 2, :].shape = {logits[:, 2, :].shape} # all batch, time=2: (2, 10)\")\n",
    "print(f\"logits[:, :, 5].shape = {logits[:, :, 5].shape} # all batch/time, vocab=5: (2, 5)\")\n",
    "print()\n",
    "\n",
    "print(\"Argmax to get token predictions:\")\n",
    "tokens = logits.argmax(dim=2)  # best vocab per timestep\n",
    "print(f\"tokens.shape = {tokens.shape}  # (2, 5) - token per time step\")\n",
    "print(f\"tokens = {tokens}\")\n",
    "print()\n",
    "\n",
    "print(\"Real Seq2Seq scenario:\")\n",
    "targets = torch.arange(2*5).reshape(2, 5) % 10  # fake target tokens\n",
    "print(f\"targets.shape = {targets.shape}\")\n",
    "print(f\"targets = {targets}\")\n",
    "print()\n",
    "\n",
    "# Reshape for loss (CrossEntropyLoss needs (N, C) not (batch, time, C))\n",
    "logits_flat = logits.reshape(-1, 10)      # (10, 10) = (batch*time, vocab)\n",
    "targets_flat = targets.reshape(-1)        # (10,) = (batch*time,)\n",
    "print(f\"After reshape for loss computation:\")\n",
    "print(f\"logits_flat.shape  = {logits_flat.shape}\")\n",
    "print(f\"targets_flat.shape = {targets_flat.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ef6d0d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## SECTION 4: PyTorch Dimension Tricks\n",
    "\n",
    "### What: Four operations to reshape tensors\n",
    "1. **`unsqueeze(dim)`**: Add a dimension of size 1\n",
    "2. **`squeeze(dim)`**: Remove dimension of size 1\n",
    "3. **`reshape(shape)` / `view(shape)`**: Change shape\n",
    "4. **`permute(dims)`**: Rearrange dimension order\n",
    "\n",
    "### Why: Layers expect specific shapes\n",
    "- LSTM: input must be `(batch, seq, features)` not `(batch, seq)`\n",
    "- Loss: needs `(N, C)` not `(batch, time, C)`\n",
    "- CNNâ†’Dense: flatten spatial dims: `(batch, C, H, W)` â†’ `(batch, C*H*W)`\n",
    "\n",
    "### Where: Everywhere in real code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "818d0bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DIMENSION TRICKS: unsqueeze, squeeze, reshape, permute\n",
      "============================================================\n",
      "\n",
      "x.shape = torch.Size([2, 3])\n",
      "\n",
      "tensor([[ 0.2135,  0.9987,  2.9061],\n",
      "        [-0.6749, -2.0914, -1.5020]])\n",
      "\n",
      "1ï¸âƒ£  UNSQUEEZE (add dimension):\n",
      "x.unsqueeze(0).shape  = torch.Size([1, 2, 3])   # (1, 2, 3) - add at start\n",
      "tensor([[[ 0.2135,  0.9987,  2.9061],\n",
      "         [-0.6749, -2.0914, -1.5020]]])\n",
      "x.unsqueeze(1).shape  = torch.Size([2, 1, 3])   # (2, 1, 3) - add in middle\n",
      "tensor([[[ 0.2135,  0.9987,  2.9061]],\n",
      "\n",
      "        [[-0.6749, -2.0914, -1.5020]]])\n",
      "x.unsqueeze(2).shape  = torch.Size([2, 3, 1])   # (2, 3, 1) - add at end\n",
      "x.unsqueeze(-1).shape = torch.Size([2, 3, 1])  # (2, 3, 1) - add at end (alternate)\n",
      "tensor([[[ 0.2135],\n",
      "         [ 0.9987],\n",
      "         [ 2.9061]],\n",
      "\n",
      "        [[-0.6749],\n",
      "         [-2.0914],\n",
      "         [-1.5020]]])\n",
      "\n",
      "2ï¸âƒ£  SQUEEZE (remove dimension of size 1):\n",
      "y.shape = torch.Size([2, 1, 3])\n",
      "tensor([[[ 0.2135,  0.9987,  2.9061]],\n",
      "\n",
      "        [[-0.6749, -2.0914, -1.5020]]])\n",
      "y.squeeze().shape = torch.Size([2, 3])       # (2, 3) - remove all size-1 dims\n",
      "tensor([[ 0.2135,  0.9987,  2.9061],\n",
      "        [-0.6749, -2.0914, -1.5020]])\n",
      "y.squeeze(1).shape = torch.Size([2, 3])     # (2, 3) - remove dim 1 specifically\n",
      "tensor([[ 0.2135,  0.9987,  2.9061],\n",
      "        [-0.6749, -2.0914, -1.5020]])\n",
      "\n",
      "3ï¸âƒ£  RESHAPE (change shape):\n",
      "z.shape = torch.Size([2, 3, 4])\n",
      "z.reshape(6, 4).shape = torch.Size([6, 4])     # flatten batch/time\n",
      "z.reshape(-1, 4).shape = torch.Size([6, 4])   # -1 means 'infer this dim'\n",
      "z.reshape(2, 12).shape = torch.Size([2, 12])   # keep batch, flatten rest\n",
      "\n",
      "4ï¸âƒ£  PERMUTE (rearrange dimensions):\n",
      "z.shape = torch.Size([2, 3, 4])  # (2, 3, 4) = (batch, time, features)\n",
      "z.permute(0, 2, 1).shape = torch.Size([2, 4, 3])  # (batch, features, time)\n",
      "z.permute(2, 0, 1).shape = torch.Size([4, 2, 3])  # (features, batch, time)\n",
      "\n",
      "5ï¸âƒ£  VIEW vs RESHAPE (same result, view requires contiguous):\n",
      "z.view(6, 4).shape = torch.Size([6, 4])\n",
      "z.reshape(6, 4).shape = torch.Size([6, 4])\n",
      "view fails if tensor is non-contiguous (e.g., after permute)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"DIMENSION TRICKS: unsqueeze, squeeze, reshape, permute\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "\n",
    "# Starting tensor\n",
    "x = torch.randn(2, 3)  # (batch=2, features=3)\n",
    "print(f\"x.shape = {x.shape}\")\n",
    "print()\n",
    "print(x)\n",
    "print()\n",
    "print(\"1ï¸âƒ£  UNSQUEEZE (add dimension):\")\n",
    "print(f\"x.unsqueeze(0).shape  = {x.unsqueeze(0).shape}   # (1, 2, 3) - add at start\")\n",
    "print(x.unsqueeze(0))\n",
    "print(f\"x.unsqueeze(1).shape  = {x.unsqueeze(1).shape}   # (2, 1, 3) - add in middle\")\n",
    "print(x.unsqueeze(1))\n",
    "print(f\"x.unsqueeze(2).shape  = {x.unsqueeze(2).shape}   # (2, 3, 1) - add at end\")\n",
    "print(f\"x.unsqueeze(-1).shape = {x.unsqueeze(-1).shape}  # (2, 3, 1) - add at end (alternate)\")\n",
    "print(x.unsqueeze(-1))\n",
    "print()\n",
    "\n",
    "print(\"2ï¸âƒ£  SQUEEZE (remove dimension of size 1):\")\n",
    "y = x.unsqueeze(1)  # now (2, 1, 3)\n",
    "print(f\"y.shape = {y.shape}\")\n",
    "print(y)\n",
    "print(f\"y.squeeze().shape = {y.squeeze().shape}       # (2, 3) - remove all size-1 dims\")\n",
    "print(y.squeeze())\n",
    "print(f\"y.squeeze(1).shape = {y.squeeze(1).shape}     # (2, 3) - remove dim 1 specifically\")\n",
    "print(y.squeeze(1))\n",
    "print()\n",
    "\n",
    "print(\"3ï¸âƒ£  RESHAPE (change shape):\")\n",
    "z = torch.arange(2*3*4).reshape(2, 3, 4)  # (2, 3, 4)\n",
    "print(f\"z.shape = {z.shape}\")\n",
    "print(f\"z.reshape(6, 4).shape = {z.reshape(6, 4).shape}     # flatten batch/time\")\n",
    "print(f\"z.reshape(-1, 4).shape = {z.reshape(-1, 4).shape}   # -1 means 'infer this dim'\")\n",
    "print(f\"z.reshape(2, 12).shape = {z.reshape(2, 12).shape}   # keep batch, flatten rest\")\n",
    "print()\n",
    "\n",
    "print(\"4ï¸âƒ£  PERMUTE (rearrange dimensions):\")\n",
    "print(f\"z.shape = {z.shape}  # (2, 3, 4) = (batch, time, features)\")\n",
    "print(f\"z.permute(0, 2, 1).shape = {z.permute(0, 2, 1).shape}  # (batch, features, time)\")\n",
    "print(f\"z.permute(2, 0, 1).shape = {z.permute(2, 0, 1).shape}  # (features, batch, time)\")\n",
    "print()\n",
    "\n",
    "print(\"5ï¸âƒ£  VIEW vs RESHAPE (same result, view requires contiguous):\")\n",
    "print(f\"z.view(6, 4).shape = {z.view(6, 4).shape}\")\n",
    "print(f\"z.reshape(6, 4).shape = {z.reshape(6, 4).shape}\")\n",
    "print(\"view fails if tensor is non-contiguous (e.g., after permute)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8f5a2f",
   "metadata": {},
   "source": [
    "### Real Seq2Seq Decoder Example\n",
    "```python\n",
    "# One token at a time during decoding\n",
    "token_id = 42              # (,) scalar token\n",
    "token_id = token_id.unsqueeze(0)  # (1,) for embedding\n",
    "embed = embedding(token_id)       # (1, embed_dim) from embedding table\n",
    "embed = embed.unsqueeze(1)        # (1, 1, embed_dim) - LSTM needs time dim!\n",
    "output, (h, c) = lstm(embed, (h, c))  # (1, 1, hidden) output\n",
    "output = output.squeeze(1)        # (1, hidden) - remove time dim\n",
    "logits = fc_layer(output)         # (1, vocab) - get predictions\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369ad879",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## SECTION 5: Cat vs Stack (Critical Difference)\n",
    "\n",
    "### What:\n",
    "- **`torch.cat()`**: Join along **existing** dimension\n",
    "- **`torch.stack()`**: Create **new** dimension\n",
    "\n",
    "### Why: Different data structures need different operations\n",
    "- **cat**: More of the same (more samples, more timesteps)\n",
    "- **stack**: Group variants (ensemble predictions, multi-model outputs)\n",
    "\n",
    "### Where: Used in\n",
    "- **cat**: Batch loading, bidirectional RNNs, concatenating features\n",
    "- **stack**: Building outputs over time, ensemble averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2d5fe4e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CAT vs STACK\n",
      "============================================================\n",
      "\n",
      "a.shape = torch.Size([2, 3])\n",
      "b.shape = torch.Size([2, 3])\n",
      "\n",
      "torch.cat() - Join along existing dimension:\n",
      "cat([a, b], dim=0).shape = torch.Size([4, 3])  # (4, 3) - more rows\n",
      "[[1, 1, 1],\n",
      " [1, 1, 1],  â† batch doubled!\n",
      " [0, 0, 0],\n",
      " [0, 0, 0]]\n",
      "\n",
      "cat([a, b], dim=1).shape = torch.Size([2, 6])  # (2, 6) - more columns\n",
      "[[1, 1, 1, 0, 0, 0],  â† features doubled!\n",
      " [1, 1, 1, 0, 0, 0]]\n",
      "\n",
      "\n",
      "torch.stack() - Create new dimension:\n",
      "stack([a, b], dim=0).shape = torch.Size([2, 2, 3])  # (2, 2, 3) - new batch dim\n",
      "\n",
      "stack([a, b], dim=1).shape = torch.Size([2, 2, 3])  # (2, 2, 3) - new middle dim\n",
      "\n",
      "stack([a, b], dim=2).shape = torch.Size([2, 3, 2])  # (2, 3, 2) - new last dim\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"CAT vs STACK\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "\n",
    "a = torch.ones(2, 3)\n",
    "b = torch.zeros(2, 3)\n",
    "print(f\"a.shape = {a.shape}\")\n",
    "print(f\"b.shape = {b.shape}\")\n",
    "print()\n",
    "\n",
    "print(\"torch.cat() - Join along existing dimension:\")\n",
    "cat_dim0 = torch.cat([a, b], dim=0)\n",
    "print(f\"cat([a, b], dim=0).shape = {cat_dim0.shape}  # (4, 3) - more rows\")\n",
    "print(\"[[1, 1, 1],\")\n",
    "print(\" [1, 1, 1],  â† batch doubled!\")\n",
    "print(\" [0, 0, 0],\")\n",
    "print(\" [0, 0, 0]]\")\n",
    "print()\n",
    "\n",
    "cat_dim1 = torch.cat([a, b], dim=1)\n",
    "print(f\"cat([a, b], dim=1).shape = {cat_dim1.shape}  # (2, 6) - more columns\")\n",
    "print(\"[[1, 1, 1, 0, 0, 0],  â† features doubled!\")\n",
    "print(\" [1, 1, 1, 0, 0, 0]]\")\n",
    "print()\n",
    "print()\n",
    "\n",
    "print(\"torch.stack() - Create new dimension:\")\n",
    "stack_dim0 = torch.stack([a, b], dim=0)\n",
    "print(f\"stack([a, b], dim=0).shape = {stack_dim0.shape}  # (2, 2, 3) - new batch dim\")\n",
    "print()\n",
    "\n",
    "stack_dim1 = torch.stack([a, b], dim=1)\n",
    "print(f\"stack([a, b], dim=1).shape = {stack_dim1.shape}  # (2, 2, 3) - new middle dim\")\n",
    "print()\n",
    "\n",
    "stack_dim2 = torch.stack([a, b], dim=2)\n",
    "print(f\"stack([a, b], dim=2).shape = {stack_dim2.shape}  # (2, 3, 2) - new last dim\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64ad2e4",
   "metadata": {},
   "source": [
    "### Seq2Seq Autoregressive Generation Example\n",
    "```python\n",
    "# Build output token-by-token (decoder generates one prediction per step)\n",
    "outputs = []  # will collect predictions at each timestep\n",
    "\n",
    "for t in range(max_time):\n",
    "    logits_t = decoder(input_t, hidden)  # (batch, vocab) for time t\n",
    "    outputs.append(logits_t)  # list of (batch, vocab) tensors\n",
    "    hidden = update_hidden(logits_t)  # update hidden state\n",
    "\n",
    "# Stack all timesteps: [(batch, vocab), ...] â†’ (batch, time, vocab)\n",
    "all_outputs = torch.stack(outputs, dim=1)  # (batch, time, vocab) âœ“\n",
    "\n",
    "# If we cat instead, wrong!\n",
    "wrong = torch.cat(outputs, dim=1)  # (batch, vocab*time) âœ—\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88ab5a2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## SECTION 6: Time Step & Batch Selection (Decoding Loops)\n",
    "\n",
    "### What: Extract specific samples/timesteps from batched tensors\n",
    "**Keep batch dimension intact** - this is key!\n",
    "\n",
    "### Why: Core in training and especially inference\n",
    "- **Training:** Sample specific batches for debugging\n",
    "- **Inference:** Decode one timestep at a time\n",
    "- **Debugging:** Inspect individual predictions\n",
    "\n",
    "### Where: Used in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "46125c16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TIME STEP & BATCH SELECTION\n",
      "============================================================\n",
      "\n",
      "logits.shape = torch.Size([4, 6, 10])  # (batch=4, time=6, vocab=10)\n",
      "\n",
      "Getting specific timesteps (decoder loop):\n",
      "logits[:, 2, :].shape = torch.Size([4, 10])  # (4, 10) - all batch at time 2\n",
      "\n",
      "Getting specific batch:\n",
      "logits[1, :, :].shape = torch.Size([6, 10])  # (6, 10) - sample 1 full sequence\n",
      "\n",
      "Getting specific batch AND timestep:\n",
      "logits[1, 2, :].shape = torch.Size([10])  # (10,) - sample 1 at time 2\n",
      "\n",
      "Argmax to get predicted tokens:\n",
      "token_ids.shape = torch.Size([4, 6])  # (4, 6) - token per batch per time\n",
      "token_ids = tensor([[2, 9, 8, 2, 1, 4],\n",
      "        [9, 3, 2, 2, 0, 0],\n",
      "        [3, 6, 0, 8, 1, 9],\n",
      "        [6, 0, 9, 8, 5, 1]])\n",
      "\n",
      "Teacher forcing next input:\n",
      "teacher_next.shape = torch.Size([4])  # (4,) - one token per batch\n",
      "teacher_next = tensor([9, 3, 6, 0])  â† use as next decoder input\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TIME STEP & BATCH SELECTION\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "\n",
    "# Seq2Seq decoder output: (batch=4, time=6, vocab=10)\n",
    "logits = torch.randn(4, 6, 10)\n",
    "print(f\"logits.shape = {logits.shape}  # (batch=4, time=6, vocab=10)\")\n",
    "print()\n",
    "\n",
    "print(\"Getting specific timesteps (decoder loop):\")\n",
    "t = 2\n",
    "logits_at_t = logits[:, t, :]  # all batches at time t\n",
    "print(f\"logits[:, {t}, :].shape = {logits_at_t.shape}  # (4, 10) - all batch at time {t}\")\n",
    "print()\n",
    "\n",
    "print(\"Getting specific batch:\")\n",
    "b = 1\n",
    "logits_batch_b = logits[b, :, :]  # sample b, all times\n",
    "print(f\"logits[{b}, :, :].shape = {logits_batch_b.shape}  # (6, 10) - sample {b} full sequence\")\n",
    "print()\n",
    "\n",
    "print(\"Getting specific batch AND timestep:\")\n",
    "logits_single = logits[b, t, :]  # sample b, time t\n",
    "print(f\"logits[{b}, {t}, :].shape = {logits_single.shape}  # (10,) - sample {b} at time {t}\")\n",
    "print()\n",
    "\n",
    "print(\"Argmax to get predicted tokens:\")\n",
    "token_ids = logits.argmax(dim=2)  # best vocab per timestep\n",
    "print(f\"token_ids.shape = {token_ids.shape}  # (4, 6) - token per batch per time\")\n",
    "print(f\"token_ids = {token_ids}\")\n",
    "print()\n",
    "\n",
    "print(\"Teacher forcing next input:\")\n",
    "time_step = 1\n",
    "teacher_next = token_ids[:, time_step]  # get predictions at time_step\n",
    "print(f\"teacher_next.shape = {teacher_next.shape}  # (4,) - one token per batch\")\n",
    "print(f\"teacher_next = {teacher_next}  â† use as next decoder input\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ee0dd9",
   "metadata": {},
   "source": [
    "### Real Seq2Seq Decoding Pattern\n",
    "```python\n",
    "logits = model(src_ids, tgt_ids)  # (batch, tgt_len, vocab)\n",
    "\n",
    "# Autoregressive: one word at a time\n",
    "for t in range(tgt_len):\n",
    "    current_step = logits[:, t, :]    # (batch, vocab) - preds at step t\n",
    "    tokens_t = current_step.argmax(dim=1)  # (batch,) - best token per sample\n",
    "    \n",
    "    if (tokens_t == EOS_ID).any():\n",
    "        print(\"Some samples finished!\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9db20a8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## SECTION 7: Masking & Padding (Critical for Training)\n",
    "\n",
    "### What: Create boolean masks to ignore PAD tokens\n",
    "**Paddings are fake, don't let them ruin your training!**\n",
    "\n",
    "### Why: Deep models must ignore padding\n",
    "- Padding tokens are just fillers to make sequences same length\n",
    "- Loss should NOT penalize predictions on padding\n",
    "- Attention should NOT focus on paddings\n",
    "- RNN should take hidden state at actual sequence end, not at padding\n",
    "\n",
    "### Where: Used in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "004b5b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MASKING & PADDING: The Right Way\n",
      "============================================================\n",
      "\n",
      "targets = tensor([[1, 2, 3, 0, 0],\n",
      "        [4, 5, 0, 0, 0]])  (0 = PAD)\n",
      "\n",
      "1ï¸âƒ£  Loss Masking (Seq2Seq):\n",
      "Loss computed: torch.Size([10])\n",
      "pad_mask: tensor([False, False, False,  True,  True, False, False,  True,  True,  True])\n",
      "Final loss (ignoring pads): 5.0860\n",
      "\n",
      "2ï¸âƒ£  Attention Masking (Transformer):\n",
      "Attention scores masked (padded positions set to -inf)\n",
      "\n",
      "3ï¸âƒ£  RNN Output Masking (Get hidden at actual end):\n",
      "âŒ Last timestep (includes padding): torch.Size([2, 4])\n",
      "âœ“ Actual last timestep (no padding): torch.Size([2, 4])\n",
      "Batch 0: used timestep 3 (real)\n",
      "Batch 1: used timestep 2 (real)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"MASKING & PADDING: The Right Way\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "\n",
    "# Targets with padding\n",
    "targets = torch.tensor([[1, 2, 3, 0, 0],     # 3 real tokens, 2 pads\n",
    "                        [4, 5, 0, 0, 0]])    # 2 real tokens, 3 pads\n",
    "PAD_ID = 0\n",
    "print(f\"targets = {targets}  (0 = PAD)\")\n",
    "print()\n",
    "\n",
    "print(\"1ï¸âƒ£  Loss Masking (Seq2Seq):\")\n",
    "logits = torch.randn(2, 5, 100)  # (batch, time, vocab)\n",
    "loss_fn = nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "# Flat for loss computation\n",
    "L = loss_fn(logits.reshape(-1, 100), targets.reshape(-1))  # (10,)\n",
    "print(f\"Loss computed: {L.shape}\")\n",
    "\n",
    "# Create mask\n",
    "pad_mask = targets.reshape(-1) == PAD_ID  # (10,) bool\n",
    "print(f\"pad_mask: {pad_mask}\")\n",
    "\n",
    "# Apply mask\n",
    "L[pad_mask] = 0  # zero out PAD losses\n",
    "final_loss = L.sum() / (~pad_mask).sum()  # avg only real tokens\n",
    "print(f\"Final loss (ignoring pads): {final_loss:.4f}\")\n",
    "print()\n",
    "\n",
    "print(\"2ï¸âƒ£  Attention Masking (Transformer):\")\n",
    "# Prevent attention to padded positions\n",
    "attn_scores = torch.randn(2, 4, 5, 5)  # (batch, heads, seq, seq)\n",
    "seq_mask = torch.tensor([1, 1, 1, 0, 0], dtype=torch.bool)  # which are real\n",
    "# Expand mask for broadcasting\n",
    "seq_mask_2d = seq_mask.unsqueeze(0).unsqueeze(0).unsqueeze(0)  # (1, 1, 1, 5)\n",
    "attn_scores.masked_fill_(~seq_mask_2d, float('-inf'))\n",
    "print(f\"Attention scores masked (padded positions set to -inf)\")\n",
    "print()\n",
    "\n",
    "print(\"3ï¸âƒ£  RNN Output Masking (Get hidden at actual end):\")\n",
    "batch_size, seq_len, hidden_dim = 2, 5, 4\n",
    "rnn_outputs = torch.randn(batch_size, seq_len, hidden_dim)\n",
    "actual_lengths = torch.tensor([3, 2])  # real sequence lengths\n",
    "\n",
    "# âŒ Wrong: use last timestep (includes padding)\n",
    "wrong = rnn_outputs[:, -1, :]\n",
    "print(f\"âŒ Last timestep (includes padding): {wrong.shape}\")\n",
    "\n",
    "# âœ“ Right: use actual last timestep\n",
    "correct = rnn_outputs[range(batch_size), actual_lengths - 1, :]\n",
    "print(f\"âœ“ Actual last timestep (no padding): {correct.shape}\")\n",
    "print(f\"Batch 0: used timestep {actual_lengths[0].item()} (real)\")\n",
    "print(f\"Batch 1: used timestep {actual_lengths[1].item()} (real)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6832d1a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## SECTION 8: Broadcasting (Automatic Shape Expansion)\n",
    "\n",
    "### What: Smaller tensors auto-expand to match larger ones\n",
    "**No copying - just reuse the same data multiple times!**\n",
    "\n",
    "### Why: Save memory, write cleaner code\n",
    "- Element-wise ops require same shapes\n",
    "- Broadcasting does the expansion for you\n",
    "- Much faster than manual loops\n",
    "\n",
    "### Broadcasting Rules (Remember these!):\n",
    "1. Compare dimensions from RIGHT to LEFT\n",
    "2. Dimensions must match OR one must be 1 (or not exist)\n",
    "3. The size-1 dimension broadcasts to match the larger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5642ea9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "BROADCASTING: Auto Shape Expansion\n",
      "============================================================\n",
      "\n",
      "1ï¸âƒ£  ANN - Add Bias:\n",
      "logits.shape: torch.Size([32, 10])\n",
      "bias.shape:   torch.Size([10])  â†’ broadcasts to (32, 10)\n",
      "output.shape: torch.Size([32, 10])\n",
      "\n",
      "2ï¸âƒ£  CNN - Batch Norm Affine:\n",
      "x.shape:           torch.Size([16, 3, 224, 224])\n",
      "gamma.shape:       torch.Size([3])  â†’ reshape to (1, 3, 1, 1)\n",
      "normalized.shape:  torch.Size([16, 3, 224, 224])\n",
      "\n",
      "3ï¸âƒ£  Transformer - Attention Mask:\n",
      "attn_scores.shape:  torch.Size([2, 4, 5, 5])\n",
      "mask.shape:         torch.Size([5, 5])  â†’ unsqueeze to (1, 1, 5, 5)\n",
      "result.shape:       torch.Size([2, 4, 5, 5])\n",
      "\n",
      "4ï¸âƒ£  Seq2Seq - Loss Weighting:\n",
      "loss.shape:          torch.Size([8, 15])\n",
      "weights.shape:       torch.Size([8, 1])  â†’ broadcasts to (8, 15)\n",
      "weighted_loss.shape: torch.Size([8, 15])\n",
      "Batch 0's weight 2.0 applied to all 15 timesteps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"BROADCASTING: Auto Shape Expansion\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "\n",
    "print(\"1ï¸âƒ£  ANN - Add Bias:\")\n",
    "logits = torch.randn(32, 10)   # (batch, vocab)\n",
    "bias = torch.randn(10)          # (vocab,)\n",
    "output = logits + bias          # bias broadcasts to (32, 10)\n",
    "print(f\"logits.shape: {logits.shape}\")\n",
    "print(f\"bias.shape:   {bias.shape}  â†’ broadcasts to (32, 10)\")\n",
    "print(f\"output.shape: {output.shape}\")\n",
    "print()\n",
    "\n",
    "print(\"2ï¸âƒ£  CNN - Batch Norm Affine:\")\n",
    "x = torch.randn(16, 3, 224, 224)  # (batch, channels, H, W)\n",
    "gamma = torch.randn(3)  # (channels,) scale\n",
    "beta = torch.randn(3)   # (channels,) shift\n",
    "\n",
    "# Reshape for broadcasting\n",
    "gamma_reshaped = gamma.view(1, 3, 1, 1)  # (1, 3, 1, 1)\n",
    "beta_reshaped = beta.view(1, 3, 1, 1)    # (1, 3, 1, 1)\n",
    "normalized = (x * gamma_reshaped) + beta_reshaped\n",
    "\n",
    "print(f\"x.shape:           {x.shape}\")\n",
    "print(f\"gamma.shape:       {gamma.shape}  â†’ reshape to (1, 3, 1, 1)\")\n",
    "print(f\"normalized.shape:  {normalized.shape}\")\n",
    "print()\n",
    "\n",
    "print(\"3ï¸âƒ£  Transformer - Attention Mask:\")\n",
    "attn_scores = torch.randn(2, 4, 5, 5)  # (batch, heads, seq, seq)\n",
    "mask = torch.ones(5, 5)  # (seq, seq) - same for all batches/heads\n",
    "mask_expanded = mask.unsqueeze(0).unsqueeze(0)  # (1, 1, 5, 5)\n",
    "masked_scores = attn_scores + mask_expanded  # broadcasts!\n",
    "\n",
    "print(f\"attn_scores.shape:  {attn_scores.shape}\")\n",
    "print(f\"mask.shape:         {mask.shape}  â†’ unsqueeze to (1, 1, 5, 5)\")\n",
    "print(f\"result.shape:       {masked_scores.shape}\")\n",
    "print()\n",
    "\n",
    "print(\"4ï¸âƒ£  Seq2Seq - Loss Weighting:\")\n",
    "loss = torch.randn(8, 15)  # (batch, seq_len)\n",
    "weights = torch.tensor([[2.0], [1.0], [1.0], [1.0],\n",
    "                        [1.5], [1.5], [1.0], [1.0]])  # (batch, 1)\n",
    "weighted_loss = loss * weights  # weights broadcast to (8, 15)\n",
    "\n",
    "print(f\"loss.shape:          {loss.shape}\")\n",
    "print(f\"weights.shape:       {weights.shape}  â†’ broadcasts to (8, 15)\")\n",
    "print(f\"weighted_loss.shape: {weighted_loss.shape}\")\n",
    "print(f\"Batch 0's weight {weights[0, 0].item():.1f} applied to all 15 timesteps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e34316f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## SECTION 9: NumPy Fundamentals (Data Preparation)\n",
    "\n",
    "### What: Core NumPy operations for preprocessing\n",
    "You'll use these to load and prepare data BEFORE feeding to PyTorch\n",
    "\n",
    "### Where: Used in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c8d04801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "NUMPY FUNDAMENTALS\n",
      "============================================================\n",
      "\n",
      "Creation:\n",
      "A:\n",
      "[[1. 2.]\n",
      " [3. 4.]]\n",
      "R shape (2, 3):\n",
      "[[0 1 2]\n",
      " [3 4 5]]\n",
      "\n",
      "Reshape/Flatten:\n",
      "R.reshape(-1): [0 1 2 3 4 5]\n",
      "R.T (transpose): shape (3, 2)\n",
      "\n",
      "Combining:\n",
      "concatenate axis=0: (4, 3)\n",
      "stack axis=0: (2, 2, 3)\n",
      "\n",
      "Reductions:\n",
      "R.sum(): 15\n",
      "R.sum(axis=0): [3 5 7]\n",
      "R.max(axis=1): [2 5]\n",
      "\n",
      "Broadcasting:\n",
      "X shape: (2, 3)\n",
      "bias shape: (3,) â†’ broadcasts\n",
      "X + bias:\n",
      "[[11. 22. 33.]\n",
      " [14. 25. 36.]]\n",
      "\n",
      "Boolean Masking:\n",
      "X > 3:\n",
      "[[False False False]\n",
      " [ True  True  True]]\n",
      "X[mask]: [4. 5. 6.]  (only values > 3)\n",
      "\n",
      "Type conversion:\n",
      "int dtype: int64\n",
      "float dtype: float32  (good for PyTorch!)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"NUMPY FUNDAMENTALS\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "\n",
    "print(\"Creation:\")\n",
    "A = np.array([[1, 2], [3, 4]], dtype=np.float32)\n",
    "Z = np.zeros((2, 3))\n",
    "O = np.ones((2, 3))\n",
    "R = np.arange(6).reshape(2, 3)  # 0..5 reshaped\n",
    "print(f\"A:\\n{A}\")\n",
    "print(f\"R shape {R.shape}:\\n{R}\")\n",
    "print()\n",
    "\n",
    "print(\"Reshape/Flatten:\")\n",
    "R_flat = R.reshape(-1)  # -1 means infer\n",
    "print(f\"R.reshape(-1): {R_flat}\")\n",
    "print(f\"R.T (transpose): shape {R.T.shape}\")\n",
    "print()\n",
    "\n",
    "print(\"Combining:\")\n",
    "A1 = np.ones((2, 3))\n",
    "A2 = np.zeros((2, 3))\n",
    "cat_result = np.concatenate([A1, A2], axis=0)  # stack vertically\n",
    "print(f\"concatenate axis=0: {cat_result.shape}\")\n",
    "stack_result = np.stack([A1, A2], axis=0)  # group\n",
    "print(f\"stack axis=0: {stack_result.shape}\")\n",
    "print()\n",
    "\n",
    "print(\"Reductions:\")\n",
    "print(f\"R.sum(): {R.sum()}\")\n",
    "print(f\"R.sum(axis=0): {R.sum(axis=0)}\")\n",
    "print(f\"R.max(axis=1): {R.max(axis=1)}\")\n",
    "print()\n",
    "\n",
    "print(\"Broadcasting:\")\n",
    "X = np.array([[1., 2., 3.], [4., 5., 6.]])  # (2, 3)\n",
    "bias = np.array([10., 20., 30.])  # (3,)\n",
    "result = X + bias  # broadcasts bias to (2, 3)\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"bias shape: {bias.shape} â†’ broadcasts\")\n",
    "print(f\"X + bias:\\n{result}\")\n",
    "print()\n",
    "\n",
    "print(\"Boolean Masking:\")\n",
    "mask = X > 3\n",
    "print(f\"X > 3:\\n{mask}\")\n",
    "print(f\"X[mask]: {X[mask]}  (only values > 3)\")\n",
    "print()\n",
    "\n",
    "print(\"Type conversion:\")\n",
    "int_arr = np.array([1, 2, 3], dtype=np.int64)\n",
    "float_arr = int_arr.astype(np.float32)\n",
    "print(f\"int dtype: {int_arr.dtype}\")\n",
    "print(f\"float dtype: {float_arr.dtype}  (good for PyTorch!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7fad4b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## SECTION 10: Architecture-Specific Deep Dives\n",
    "\n",
    "Now that you understand indexing fundamentals, let's see how each architecture uses these patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45741f10",
   "metadata": {},
   "source": [
    "### 10.1: ANN (Fully Connected Networks)\n",
    "Shape: `(batch, features)` â†’ `(batch, hidden)` â†’ `(batch, classes)`\n",
    "\n",
    "**Key indexing patterns:**\n",
    "- Batch: `x[i, :]` get sample i\n",
    "- Features: `x[:, j]` get feature j across batch\n",
    "- Classes: `logits[:, c]` get probability of class c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "abbf38d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ANN (Dense Networks) - Indexing Patterns\n",
      "============================================================\n",
      "\n",
      "Input x.shape: torch.Size([32, 784])\n",
      "Hidden.shape: torch.Size([32, 128])\n",
      "Logits.shape: torch.Size([32, 10])\n",
      "\n",
      "Key indexing operations:\n",
      "Sample 0 logits: torch.Size([10])  # (10,) all class scores for sample 0\n",
      "Class 5 scores: torch.Size([32])  # (32,) class 5 score for all samples\n",
      "\n",
      "Predictions: torch.Size([32])  values: tensor([5, 6, 2, 4, 6])\n",
      "\n",
      "Loss: 2.3720\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ANN (Dense Networks) - Indexing Patterns\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "\n",
    "# Input: 32 images, 784 pixels each (28x28 MNIST flattened)\n",
    "x = torch.randn(32, 784)     # (batch, input_features)\n",
    "print(f\"Input x.shape: {x.shape}\")\n",
    "\n",
    "# Forward pass\n",
    "fc1 = nn.Linear(784, 128)\n",
    "fc2 = nn.Linear(128, 10)     # 10 classes\n",
    "\n",
    "h = torch.relu(fc1(x))       # (32, 128)\n",
    "logits = fc2(h)              # (32, 10)\n",
    "print(f\"Hidden.shape: {h.shape}\")\n",
    "print(f\"Logits.shape: {logits.shape}\")\n",
    "print()\n",
    "\n",
    "print(\"Key indexing operations:\")\n",
    "print(f\"Sample 0 logits: {logits[0].shape}  # (10,) all class scores for sample 0\")\n",
    "print(f\"Class 5 scores: {logits[:, 5].shape}  # (32,) class 5 score for all samples\")\n",
    "print()\n",
    "\n",
    "# Predictions\n",
    "probs = torch.softmax(logits, dim=1)  # (32, 10)\n",
    "predictions = probs.argmax(dim=1)     # (32,) best class per sample\n",
    "print(f\"Predictions: {predictions.shape}  values: {predictions[:5]}\")\n",
    "print()\n",
    "\n",
    "# Loss\n",
    "targets = torch.randint(0, 10, (32,))\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "loss = loss_fn(logits, targets)\n",
    "print(f\"Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd816a8e",
   "metadata": {},
   "source": [
    "### 10.2: CNN (Convolutional Networks)\n",
    "Shape: `(batch, channels, height, width)` â†’ `(batch, filters, H', W')`\n",
    "\n",
    "**Key indexing patterns:**\n",
    "- Channel: `x[:, c, :, :]` get channel c\n",
    "- Spatial: `x[:, :, h:h+k, w:w+k]` get patch\n",
    "- Pixel: `x[:, :, h, w]` get all channels at pixel (h, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6dffa9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"CNN (Images) - Indexing Patterns\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "\n",
    "# Input: 8 RGB images, 224x224\n",
    "x = torch.randn(8, 3, 224, 224)  # (batch, channels, height, width)\n",
    "print(f\"Input x.shape: {x.shape}\")\n",
    "print()\n",
    "\n",
    "# Forward pass\n",
    "conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "pool = nn.MaxPool2d(2)\n",
    "\n",
    "x = conv1(x)         # (8, 64, 224, 224)\n",
    "x = torch.relu(x)\n",
    "x = pool(x)          # (8, 64, 112, 112)\n",
    "print(f\"After conv+pool: {x.shape}\")\n",
    "print()\n",
    "\n",
    "print(\"Key indexing operations:\")\n",
    "print(f\"Channel 0: {x[:, 0, :, :].shape}  # (8, 112, 112) - first filter's output\")\n",
    "print(f\"RGB channels (input): {x.shape}  # all 3 color channels\")\n",
    "print(f\"Pixel at (50, 50): {x[:, :, 50, 50].shape}  # (8, 64) - all filters at that pixel\")\n",
    "print(f\"Patch (50:55, 50:55): {x[:, :, 50:55, 50:55].shape}  # (8, 64, 5, 5) - 5x5 patch\")\n",
    "print()\n",
    "\n",
    "# Flatten for classification\n",
    "x_flat = x.reshape(8, -1)  # (8, 64*112*112)\n",
    "print(f\"Flattened: {x_flat.shape}\")\n",
    "\n",
    "fc = nn.Linear(64*112*112, 10)\n",
    "logits = fc(x_flat)  # (8, 10)\n",
    "print(f\"Logits: {logits.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab7a264",
   "metadata": {},
   "source": [
    "### 10.3: RNN/LSTM/GRU (Sequences)\n",
    "Shape: `(batch, seq_len, input)` â†’ `(batch, seq_len, hidden)` â†’ `(batch, hidden)`\n",
    "\n",
    "**Key indexing patterns:**\n",
    "- Time step: `outputs[:, t, :]` all batch at time t\n",
    "- Final: `outputs[:, -1, :]` final hidden state\n",
    "- LSTM state: `(h_n, c_n)` both needed for next step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41899f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"RNN/LSTM/GRU - Indexing Patterns\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "\n",
    "# Input: 16 sequences, length 50, embedding dimension 100\n",
    "x = torch.randn(16, 50, 100)  # (batch, seq_len, input_dim)\n",
    "print(f\"Input x.shape: {x.shape}\")\n",
    "print()\n",
    "\n",
    "# LSTM: batch=16, seq_len=50, hidden_dim=256\n",
    "lstm = nn.LSTM(100, 256, batch_first=True)\n",
    "outputs, (h_n, c_n) = lstm(x)\n",
    "print(f\"outputs.shape: {outputs.shape}  # (batch, seq_len, hidden)\")\n",
    "print(f\"h_n.shape: {h_n.shape}  # (num_layers, batch, hidden)\")\n",
    "print(f\"c_n.shape: {c_n.shape}  # cell state\")\n",
    "print()\n",
    "\n",
    "print(\"Key indexing operations:\")\n",
    "t = 10\n",
    "print(f\"outputs[:, {t}, :].shape: {outputs[:, t, :].shape}  # all batch at time {t}\")\n",
    "print(f\"outputs[:, -1, :].shape: {outputs[:, -1, :].shape}  # final output (or use h_n[0])\")\n",
    "print(f\"h_n[0].shape: {h_n[0].shape}  # final hidden state\")\n",
    "print()\n",
    "\n",
    "# Classification: use final hidden state\n",
    "final_hidden = outputs[:, -1, :]  # (16, 256) OR h_n[0]\n",
    "fc = nn.Linear(256, 10)  # 10 classes\n",
    "logits = fc(final_hidden)  # (16, 10)\n",
    "print(f\"Classification logits: {logits.shape}\")\n",
    "print()\n",
    "\n",
    "# Variable length sequences: gather at actual end\n",
    "print(\"Variable length example:\")\n",
    "actual_lengths = torch.tensor([30, 25, 50, 40, 50, 20, 45, 50,\n",
    "                               30, 25, 50, 40, 50, 20, 45, 50])\n",
    "batch_indices = torch.arange(16)\n",
    "final_correct = outputs[batch_indices, actual_lengths - 1, :]\n",
    "print(f\"Using actual_lengths: {final_correct.shape}  (correct, no padding!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b77057",
   "metadata": {},
   "source": [
    "### 10.4: Seq2Seq (Encoder-Decoder)\n",
    "Shape:\n",
    "- Encoder: `(batch, src_len)` â†’ `(batch, src_len, hidden)` + context `(batch, hidden)`\n",
    "- Decoder: `(batch, tgt_len)` â†’ `(batch, tgt_len, vocab)`\n",
    "\n",
    "**Key indexing patterns:**\n",
    "- Teacher forcing: `logits[:, t, :]` predictions at time t\n",
    "- Loss: flatten to `(batch*tgt_len, vocab)` before loss\n",
    "- Inference: argmax over vocab dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7d90ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"SEQ2SEQ (Encoder-Decoder) - Indexing Patterns\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "\n",
    "# Pretend encoder/decoder (for demo)\n",
    "batch, src_len, tgt_len, vocab = 4, 20, 15, 5000\n",
    "\n",
    "# Encoder output\n",
    "enc_outputs = torch.randn(batch, src_len, 256)  # (batch, src_len, hidden)\n",
    "context = enc_outputs[:, -1, :]  # (batch, hidden) - final state\n",
    "print(f\"enc_outputs.shape: {enc_outputs.shape}\")\n",
    "print(f\"context.shape: {context.shape}\")\n",
    "print()\n",
    "\n",
    "# Decoder output\n",
    "dec_logits = torch.randn(batch, tgt_len, vocab)  # (batch, tgt_len, vocab)\n",
    "print(f\"dec_logits.shape: {dec_logits.shape}\")\n",
    "print()\n",
    "\n",
    "print(\"Training (teacher forcing):\")\n",
    "targets = torch.randint(0, vocab, (batch, tgt_len))\n",
    "\n",
    "# Reshape for loss\n",
    "dec_logits_flat = dec_logits.reshape(-1, vocab)  # (batch*tgt_len, vocab)\n",
    "targets_flat = targets.reshape(-1)  # (batch*tgt_len,)\n",
    "print(f\"logits_flat: {dec_logits_flat.shape}\")\n",
    "print(f\"targets_flat: {targets_flat.shape}\")\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "loss = loss_fn(dec_logits_flat, targets_flat)\n",
    "print(f\"Loss: {loss.item():.4f}\")\n",
    "print()\n",
    "\n",
    "print(\"Inference (autoregressive decoding):\")\n",
    "print(\"Generate one token at a time:\")\n",
    "print(f\"At timestep t: dec_logits[:, t, :].shape = (4, 5000)\")\n",
    "print(f\"Argmax to get token: .argmax(dim=1) â†’ (4,) token per sample\")\n",
    "print()\n",
    "\n",
    "# Simulate decoding\n",
    "pred_tokens = dec_logits.argmax(dim=2)  # (batch, tgt_len) all predictions\n",
    "print(f\"All predictions: {pred_tokens.shape}\")\n",
    "print(f\"First sequence predictions: {pred_tokens[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d32208f",
   "metadata": {},
   "source": [
    "### 10.5: Transformer (Multi-Head Attention)\n",
    "Shape: `(batch, seq, dim)` â†’ `(batch, heads, seq, seq)` attention â†’ `(batch, seq, dim)`\n",
    "\n",
    "**Key indexing patterns:**\n",
    "- Head: `attn[:, h, :, :]` attention for head h\n",
    "- Position: `attn[:, :, i, :]` what position i attends to\n",
    "- Mask: broadcast causal mask for autoregressive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094a7067",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TRANSFORMER (Multi-Head Attention) - Indexing Patterns\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "\n",
    "batch, seq_len, d_model, num_heads = 8, 100, 512, 8\n",
    "head_dim = d_model // num_heads  # 64\n",
    "\n",
    "print(f\"Input: (batch={batch}, seq_len={seq_len}, d_model={d_model})\")\n",
    "print(f\"Number of heads: {num_heads}, Head dimension: {head_dim}\")\n",
    "print()\n",
    "\n",
    "# Q, K, V after projection\n",
    "Q = torch.randn(batch, seq_len, d_model).view(batch, seq_len, num_heads, head_dim)\n",
    "Q = Q.transpose(1, 2)  # (batch, heads, seq_len, head_dim)\n",
    "print(f\"Q after reshape & transpose: {Q.shape}\")\n",
    "print()\n",
    "\n",
    "K = torch.randn(batch, seq_len, d_model).view(batch, seq_len, num_heads, head_dim)\n",
    "K = K.transpose(1, 2)  # (batch, heads, seq_len, head_dim)\n",
    "\n",
    "# Attention scores\n",
    "scores = (Q @ K.transpose(-2, -1)) / math.sqrt(head_dim)  # (batch, heads, seq, seq)\n",
    "print(f\"Attention scores: {scores.shape}  # (batch, heads, seq, seq)\")\n",
    "print()\n",
    "\n",
    "print(\"Key indexing operations:\")\n",
    "h = 0\n",
    "print(f\"Head {h} attention matrix: scores[:, {h}, :, :].shape = (8, 100, 100)\")\n",
    "print(f\"What position 50 attends to: scores[:, :, 50, :].shape = (8, 8, 100)\")\n",
    "print()\n",
    "\n",
    "print(\"Causal masking for decoder (prevent looking at future):\")\n",
    "causal_mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()\n",
    "scores = scores.masked_fill(causal_mask, float('-inf'))\n",
    "print(f\"Applied causal mask (upper triangle = -inf)\")\n",
    "print()\n",
    "\n",
    "# Softmax\n",
    "weights = torch.softmax(scores, dim=-1)  # (batch, heads, seq, seq)\n",
    "print(f\"Attention weights: {weights.shape}\")\n",
    "\n",
    "# Apply to values\n",
    "V = torch.randn(batch, seq_len, d_model).view(batch, seq_len, num_heads, head_dim)\n",
    "V = V.transpose(1, 2)  # (batch, heads, seq_len, head_dim)\n",
    "\n",
    "output = weights @ V  # (batch, heads, seq_len, head_dim)\n",
    "print(f\"After attention: {output.shape}\")\n",
    "\n",
    "# Merge heads back\n",
    "output = output.transpose(1, 2).contiguous()  # (batch, seq, heads, head_dim)\n",
    "output = output.view(batch, seq_len, d_model)  # (batch, seq, d_model)\n",
    "print(f\"Merged heads: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6212da1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## SECTION 11: Common Patterns Reference\n",
    "\n",
    "Quick lookup for patterns you see in real code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3345380",
   "metadata": {},
   "source": [
    "### Pattern 1: Batch Extraction\n",
    "```python\n",
    "# Get batch i from dataset\n",
    "batch_start = i * batch_size\n",
    "batch_end = (i + 1) * batch_size\n",
    "X_batch = X[batch_start:batch_end]  # (batch_size, ...)\n",
    "y_batch = y[batch_start:batch_end]  # (batch_size,)\n",
    "```\n",
    "\n",
    "### Pattern 2: Teacher Forcing (Seq2Seq)\n",
    "```python\n",
    "# Shift targets: input is all-but-last, target is all-but-first\n",
    "dec_input = targets[:, :-1]   # remove last token\n",
    "dec_target = targets[:, 1:]   # remove first token\n",
    "# Both shape (batch, seq_len-1)\n",
    "```\n",
    "\n",
    "### Pattern 3: Loss Reshaping\n",
    "```python\n",
    "# Flatten for CrossEntropyLoss\n",
    "logits = torch.randn(batch, seq, vocab)      # (B, T, V)\n",
    "targets = torch.randint(0, vocab, (batch, seq))  # (B, T)\n",
    "\n",
    "logits_flat = logits.reshape(-1, vocab)      # (B*T, V)\n",
    "targets_flat = targets.reshape(-1)           # (B*T,)\n",
    "\n",
    "loss = criterion(logits_flat, targets_flat)  # scalar\n",
    "```\n",
    "\n",
    "### Pattern 4: Argmax â†’ Next Input\n",
    "```python\n",
    "# Get prediction tokens for next step\n",
    "logits = torch.randn(batch, vocab)  # (B, V)\n",
    "tokens = logits.argmax(dim=1)       # (B,) best token per sample\n",
    "# Use tokens as next input to decoder\n",
    "```\n",
    "\n",
    "### Pattern 5: Variable Length Handling\n",
    "```python\n",
    "# Get final hidden state at actual sequence end (not padding)\n",
    "rnn_outputs = torch.randn(batch, max_len, hidden)\n",
    "lengths = torch.tensor([30, 25, 50])  # actual lengths\n",
    "\n",
    "batch_idx = torch.arange(batch)\n",
    "final_hidden = rnn_outputs[batch_idx, lengths - 1, :]  # (batch, hidden)\n",
    "```\n",
    "\n",
    "### Pattern 6: Padding Mask Application\n",
    "```python\n",
    "# Create and apply padding mask\n",
    "targets = torch.tensor([[1, 2, 3, 0, 0], [4, 5, 0, 0, 0]])\n",
    "pad_mask = (targets == PAD_ID)  # boolean mask\n",
    "\n",
    "loss = criterion(logits.reshape(-1, vocab), targets.reshape(-1))\n",
    "loss = loss.reshape(batch, seq)\n",
    "loss = loss.masked_fill(pad_mask, 0)  # zero out pad losses\n",
    "loss = loss.sum() / (~pad_mask).sum()  # average non-pad\n",
    "```\n",
    "\n",
    "### Pattern 7: Multi-Head Attention\n",
    "```python\n",
    "# Split into heads and process\n",
    "batch, seq, d_model, num_heads = 8, 100, 512, 8\n",
    "head_dim = d_model // num_heads\n",
    "\n",
    "x = torch.randn(batch, seq, d_model)  # (8, 100, 512)\n",
    "x = x.view(batch, seq, num_heads, head_dim)  # (8, 100, 8, 64)\n",
    "x = x.transpose(1, 2)  # (8, 8, 100, 64) = (batch, heads, seq, dim_per_head)\n",
    "\n",
    "# Process each head in parallel\n",
    "# ...\n",
    "\n",
    "# Merge heads back\n",
    "x = x.transpose(1, 2).contiguous()  # (8, 100, 8, 64)\n",
    "x = x.view(batch, seq, d_model)    # (8, 100, 512)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5f12c2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## SECTION 12: Practice Exercises\n",
    "\n",
    "Test your understanding!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc1eb2b",
   "metadata": {},
   "source": [
    "### Exercise 1: Create and Slice\n",
    "```python\n",
    "import torch\n",
    "\n",
    "# Create tensor shape (3, 4, 5)\n",
    "x = torch.randn(3, 4, 5)\n",
    "\n",
    "# Task: Get all batches, last two time steps, first 2 vocab\n",
    "# Expected shape: (3, 2, 2)\n",
    "result = x[:, -2:, :2]\n",
    "print(f\"Result shape: {result.shape}\")\n",
    "```\n",
    "\n",
    "### Exercise 2: Argmax and Index\n",
    "```python\n",
    "# Get token predictions at specific timestep\n",
    "logits = torch.randn(2, 6, 10)  # (batch, time, vocab)\n",
    "tokens = logits.argmax(dim=2)   # (2, 6)\n",
    "\n",
    "# Get predictions at timestep 3 for all batches\n",
    "tokens_t3 = tokens[:, 3]  # (2,)\n",
    "print(f\"Tokens at t=3: {tokens_t3}\")\n",
    "```\n",
    "\n",
    "### Exercise 3: Padding Mask\n",
    "```python\n",
    "targets = torch.tensor([[1, 2, 3, 0, 0],\n",
    "                        [4, 5, 0, 0, 0]])\n",
    "PAD_ID = 0\n",
    "\n",
    "# Create mask and apply to logits\n",
    "pad_mask = (targets == PAD_ID)  # (2, 5) bool\n",
    "logits = torch.randn(2, 5, 7)\n",
    "logits_masked = logits.masked_fill(pad_mask.unsqueeze(-1), -float('inf'))\n",
    "print(f\"Masked shape: {logits_masked.shape}\")\n",
    "```\n",
    "\n",
    "### Exercise 4: Reshape for Loss\n",
    "```python\n",
    "# Flatten (batch, time, vocab) â†’ (batch*time, vocab)\n",
    "logits = torch.randn(4, 10, 50)  # (batch, time, vocab)\n",
    "targets = torch.randint(0, 50, (4, 10))\n",
    "\n",
    "logits_flat = logits.reshape(-1, 50)  # (40, 50)\n",
    "targets_flat = targets.reshape(-1)     # (40,)\n",
    "print(f\"Flat shapes: {logits_flat.shape}, {targets_flat.shape}\")\n",
    "```\n",
    "\n",
    "### Exercise 5: Cat vs Stack\n",
    "```python\n",
    "import torch\n",
    "\n",
    "a = torch.ones(2, 3)\n",
    "b = torch.zeros(2, 3)\n",
    "\n",
    "# What's the shape?\n",
    "c1 = torch.cat([a, b], dim=0)   # ?\n",
    "c2 = torch.cat([a, b], dim=1)   # ?\n",
    "s1 = torch.stack([a, b], dim=0) # ?\n",
    "s2 = torch.stack([a, b], dim=2) # ?\n",
    "\n",
    "print(f\"cat dim=0: {c1.shape}\")\n",
    "print(f\"cat dim=1: {c2.shape}\")\n",
    "print(f\"stack dim=0: {s1.shape}\")\n",
    "print(f\"stack dim=2: {s2.shape}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ee123a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary: One-Liner Reference\n",
    "\n",
    "| Concept | Pattern | Output Shape |\n",
    "|---------|---------|---------------|\n",
    "| **Slice** | `x[start:stop:step]` | shape depends on indices |\n",
    "| **Batch item** | `x[i]` | all dims except first |\n",
    "| **Time step** | `x[:, t, :]` | (batch, features) |\n",
    "| **Argmax** | `x.argmax(dim=d)` | shape without dim d |\n",
    "| **Unsqueeze** | `x.unsqueeze(d)` | (shape with 1 added at d) |\n",
    "| **Squeeze** | `x.squeeze(d)` | (shape with d=1 removed) |\n",
    "| **Reshape** | `x.reshape(-1, C)` | (-1 inferred as batch*time) |\n",
    "| **Permute** | `x.permute(p0, p1, ...)` | reordered dims |\n",
    "| **Cat** | `cat([a, b], dim=d)` | (shape with growth along d) |\n",
    "| **Stack** | `stack([a, b], dim=d)` | (shape with new dim at d) |\n",
    "| **Masked fill** | `x.masked_fill(mask, val)` | same shape, masked vals replaced |\n",
    "| **Broadcast** | `x + bias` | shape of larger tensor |\n",
    "\n",
    "---\n",
    "\n",
    "**You're ready! Go build awesome deep learning models!** ðŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
