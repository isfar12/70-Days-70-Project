{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "136b8362",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, sequences, labels):\n",
    "        self.sequences = torch.tensor(sequences, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.sequences[index], self.labels[index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aff6996f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = [\n",
    "    [1, 3, 2, 6],\n",
    "    [4, 1, 5, 0],\n",
    "    [2, 1, 3, 0],\n",
    "    [1, 2, 3, 0],\n",
    "    [1, 2, 4, 0]\n",
    "]\n",
    "\n",
    "labels = [0, 0, 0, 0, 1]  # 0=Even, 1=Odd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "372779bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SequenceDataset(sequences, labels)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4f1edcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[1., 2., 3., 0.],\n",
       "         [2., 1., 3., 0.]]),\n",
       " tensor([0, 0])]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e002df28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(RNNClassifier, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.LSTM(input_size=input_size,hidden_size=hidden_size,batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "    def forward(self, x):\n",
    "        print(\"‚û°Ô∏è FORWARD PASS STARTED\")\n",
    "        print(\"Input x shape (batch_size, seq_len):\", x.shape)\n",
    "        print(\"Input x:\", x)\n",
    "\n",
    "        # Add feature dimension\n",
    "        print(\"Unsqueezing input to add feature dimension\")\n",
    "        x = x.unsqueeze(-1)\n",
    "        print(\"After unsqueeze\")\n",
    "        print(\"x shape (batch_size, seq_len, input_size):\", x.shape)\n",
    "\n",
    "        # RNN\n",
    "        rnn_output, hidden_state = self.rnn(x)\n",
    "\n",
    "        print(\"üîÅ RNN OUTPUT\")\n",
    "        print(\"rnn_output shape (batch_size, seq_len, hidden_size):\", rnn_output.shape)\n",
    "        print(\"hidden_state shape (num_layers, batch_size, hidden_size):\", hidden_state[0].shape)\n",
    "\n",
    "        # Last hidden state \n",
    "        print(\"Extracting last hidden state for classification\")\n",
    "        last_hidden = hidden_state[0].squeeze(0) # squeeze num_layers dimension\n",
    "        print(\"üß† Last hidden state\")\n",
    "        print(\"last_hidden shape (batch_size, hidden_size):\", last_hidden.shape)\n",
    "\n",
    "        # Fully connected layer\n",
    "        output = self.fc(last_hidden)\n",
    "        print(\"üì§ Output logits\")\n",
    "        print(\"output shape (batch_size, num_classes):\", output.shape)\n",
    "        print(\"output:\", output)\n",
    "        print(\"‚û°Ô∏è FORWARD PASS ENDED\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "        return output\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4085081",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîß Understanding `unsqueeze(-1)`\n",
    "\n",
    "### The Problem\n",
    "Your sequences come in as raw numbers:\n",
    "```python\n",
    "x shape: (batch_size, seq_len) = (2, 4)\n",
    "x = [[1, 3, 2, 6],\n",
    "     [4, 1, 5, 0]]\n",
    "```\n",
    "\n",
    "Each token is just a **single number** with no explicit features.\n",
    "\n",
    "### LSTM's Requirement\n",
    "LSTM expects: `(batch_size, seq_len, input_features)`\n",
    "\n",
    "It needs to know:\n",
    "- How many sequences in the batch?\n",
    "- How long is each sequence?\n",
    "- How many **features** does each token have?\n",
    "\n",
    "### What `unsqueeze(-1)` Does\n",
    "Adds a new dimension at the **end** (index -1):\n",
    "\n",
    "```python\n",
    "Before: (2, 4)\n",
    "          ‚Üì\n",
    "After:  (2, 4, 1)\n",
    "          ‚îÇ  ‚îÇ  ‚îî‚îÄ 1 feature per token\n",
    "          ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ 4 tokens per sequence\n",
    "          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 2 sequences in batch\n",
    "```\n",
    "\n",
    "### Visual Example\n",
    "```python\n",
    "# Before unsqueeze\n",
    "x = [[1, 3, 2, 6],\n",
    "     [4, 1, 5, 0]]\n",
    "# Shape: (2, 4) - just 2D array\n",
    "\n",
    "# After unsqueeze(-1)\n",
    "x = [[[1], [3], [2], [6]],\n",
    "     [[4], [1], [5], [0]]]\n",
    "# Shape: (2, 4, 1) - each token now wrapped in a list\n",
    "```\n",
    "\n",
    "### Why?\n",
    "- LSTM processes **sequences with multiple features** (like embeddings)\n",
    "- Your raw tokens are treated as \"1-dimensional features\"\n",
    "- The `input_size=1` parameter confirms LSTM expects 1 feature per token\n",
    "\n",
    "### Common Pattern\n",
    "```python\n",
    "# If you had embeddings (256-d vectors per token):\n",
    "# Shape would be (batch, seq_len, 256) - no unsqueeze needed\n",
    "\n",
    "# But with raw tokens:\n",
    "# Shape is (batch, seq_len) - need unsqueeze to get (batch, seq_len, 1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e29f83",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîÑ LSTM Output: `rnn_output` vs `hidden_state`\n",
    "\n",
    "When you call LSTM, it returns **two things**:\n",
    "```python\n",
    "rnn_output, (hidden_state, cell_state) = self.lstm(x)\n",
    "```\n",
    "\n",
    "### `rnn_output` - ALL Hidden States\n",
    "```python\n",
    "Shape: (batch_size, seq_len, hidden_size) = (2, 4, 16)\n",
    "```\n",
    "\n",
    "Hidden state at **every time step**:\n",
    "```\n",
    "Time 0: hidden_0 = [0.23, -0.45, ..., 0.12]  (16 values)\n",
    "Time 1: hidden_1 = [0.55, -0.12, ..., 0.88]  (16 values)\n",
    "Time 2: hidden_2 = [0.01, 0.34, ..., -0.56]  (16 values)\n",
    "Time 3: hidden_3 = [-0.23, 0.67, ..., 0.42]  (16 values)\n",
    "\n",
    "For each of 2 sequences in the batch!\n",
    "```\n",
    "\n",
    "**When to use:** You want intermediate representations at each step\n",
    "- Machine translation (attention mechanism)\n",
    "- Named entity recognition (classify each token)\n",
    "\n",
    "### `hidden_state` - FINAL Hidden State\n",
    "```python\n",
    "Shape: (num_layers, batch_size, hidden_size) = (1, 2, 16)\n",
    "```\n",
    "\n",
    "Hidden state at the **last time step only**:\n",
    "```\n",
    "hidden_state[0] = [[-0.23, 0.67, ..., 0.42],    # Sequence 1's final state\n",
    "                   [ 0.81, -0.15, ..., 0.33]]   # Sequence 2's final state\n",
    "```\n",
    "\n",
    "**When to use:** You want a single summary of the entire sequence\n",
    "- Classification (summarize whole sentence)\n",
    "- Seq2seq encoder (compress article ‚Üí pass to decoder)\n",
    "\n",
    "### `cell_state` - LSTM Memory\n",
    "```python\n",
    "Shape: (num_layers, batch_size, hidden_size) = (1, 2, 16)\n",
    "```\n",
    "\n",
    "The internal \"memory\" that LSTM maintains.\n",
    "\n",
    "**When to use:**\n",
    "- Seq2seq: Pass both `hidden_state` and `cell_state` from encoder ‚Üí decoder\n",
    "- Classification: Usually ignore it (not needed for final prediction)\n",
    "\n",
    "### In Your Code\n",
    "You use **`hidden_state[0]`** for classification because:\n",
    "- You want one summary vector per sequence\n",
    "- `hidden_state[0]` extracts the final hidden state of the single layer\n",
    "- `squeeze(0)` removes the layer dimension ‚Üí `(2, 16)`\n",
    "- Pass to FC layer ‚Üí predict class for each sequence\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Why `hidden_state[0]` specifically?\n",
    "\n",
    "LSTM returns states with shape `(num_layers, batch_size, hidden_size)`.\n",
    "\n",
    "**Your model:** 1 layer, unidirectional\n",
    "```python\n",
    "self.rnn = nn.LSTM(input_size=1, hidden_size=16, batch_first=True)\n",
    "```\n",
    "\n",
    "So `hidden_state` has shape `(1, 2, 16)`:\n",
    "- Dimension 0: Which layer? Only 1 layer ‚Üí only index 0\n",
    "- Dimension 1: Which sequence in batch? 2 sequences ‚Üí indices 0, 1\n",
    "- Dimension 2: Which hidden value? 16 hidden units ‚Üí indices 0-15\n",
    "\n",
    "**`hidden_state[0]`** selects:\n",
    "```python\n",
    "hidden_state[0]  ‚Üí  (2, 16)  # All batch sequences, final layer\n",
    "```\n",
    "\n",
    "Then **`squeeze(0)`** removes the now-redundant layer dimension:\n",
    "```python\n",
    "hidden_state[0].squeeze(0)  ‚Üí  (2, 16)  # Ready for FC layer!\n",
    "```\n",
    "\n",
    "**If you had 2 layers:**\n",
    "```python\n",
    "self.rnn = nn.LSTM(input_size=1, hidden_size=16, num_layers=2, batch_first=True)\n",
    "```\n",
    "- `hidden_state[0]` = first layer's final state ‚Üí `(2, 16)`\n",
    "- `hidden_state[1]` = second layer's final state ‚Üí `(2, 16)`\n",
    "- **Common pattern:** Use last layer only ‚Üí `hidden_state[-1]`\n",
    "\n",
    "---\n",
    "\n",
    "## üì§ FC Layer - Classification Head\n",
    "\n",
    "```python\n",
    "self.fc = nn.Linear(hidden_size=16, num_classes=2)\n",
    "```\n",
    "\n",
    "Takes the compressed sequence summary and outputs class scores:\n",
    "\n",
    "```python\n",
    "Input:  (batch_size, hidden_size) = (2, 16)\n",
    "        [[0.23, -0.45, ..., 0.12],     # Sequence 1's summary\n",
    "         [0.55, -0.12, ..., 0.88]]     # Sequence 2's summary\n",
    "\n",
    "FC Layer (learned weights):\n",
    "        Transforms 16 ‚Üí 2\n",
    "\n",
    "Output: (batch_size, num_classes) = (2, 2)\n",
    "        [[0.234, -0.402],              # Logits for [class 0, class 1]\n",
    "         [0.507, -0.579]]              # Logits for [class 0, class 1]\n",
    "\n",
    "Interpretation:\n",
    "        Seq 1: Class 0 more likely (0.234 > -0.402)\n",
    "        Seq 2: Class 0 more likely (0.507 > -0.579)\n",
    "```\n",
    "\n",
    "**How it works:** Each output is a weighted sum:\n",
    "```\n",
    "output[0] = w[0,0]*hidden[0] + w[0,1]*hidden[1] + ... + b[0]\n",
    "output[1] = w[1,0]*hidden[0] + w[1,1]*hidden[1] + ... + b[1]\n",
    "```\n",
    "\n",
    "These logits are then passed to `CrossEntropyLoss` which:\n",
    "1. Applies softmax to convert to probabilities\n",
    "2. Compares with ground truth labels\n",
    "3. Computes loss\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Complete Forward Pass Flow\n",
    "\n",
    "```\n",
    "Input Sequences:     (2, 4)\n",
    "        ‚Üì\n",
    "unsqueeze(-1):       (2, 4, 1)\n",
    "        ‚Üì\n",
    "LSTM Processing:     Process each token through LSTM, remembering previous states\n",
    "        ‚Üì\n",
    "rnn_output:          (2, 4, 16)  ‚Üê All hidden states\n",
    "hidden_state:        (1, 2, 16)  ‚Üê Final hidden state\n",
    "        ‚Üì\n",
    "Extract final:       hidden_state[0] ‚Üí (2, 16)\n",
    "        ‚Üì\n",
    "squeeze(0):          (2, 16)  ‚Üê Ready for classification\n",
    "        ‚Üì\n",
    "FC Layer:            (2, 16) ‚Üí (2, 2)\n",
    "        ‚Üì\n",
    "Output Logits:       (2, 2)  ‚Üê Class predictions!\n",
    "```\n",
    "\n",
    "For classification:\n",
    "- We only care about the **final summary** (hidden_state)\n",
    "- We throw away **intermediate steps** (rnn_output)\n",
    "- We compress **16 dimensions ‚Üí 2 class scores**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8759dcef",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNNClassifier(input_size=1,hidden_size=16,num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00f78ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cdc25bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " EPOCH 1/2\n",
      "============================================================\n",
      "\n",
      " Batch 1\n",
      "Batch sequences shape: torch.Size([2, 4])\n",
      "Batch labels: tensor([0, 0])\n",
      "‚û°Ô∏è FORWARD PASS STARTED\n",
      "Input x shape (batch_size, seq_len): torch.Size([2, 4])\n",
      "Input x: tensor([[4., 1., 5., 0.],\n",
      "        [1., 2., 3., 0.]])\n",
      "After unsqueeze\n",
      "x shape (batch_size, seq_len, input_size): torch.Size([2, 4, 1])\n",
      "üîÅ RNN OUTPUT\n",
      "rnn_output shape (batch_size, seq_len, hidden_size): torch.Size([2, 4, 16])\n",
      "hidden_state shape (num_layers, batch_size, hidden_size): torch.Size([1, 2, 16])\n",
      "üß† Last hidden state\n",
      "last_hidden shape (batch_size, hidden_size): torch.Size([2, 16])\n",
      "üì§ Output logits\n",
      "output shape (batch_size, num_classes): torch.Size([2, 2])\n",
      "output: tensor([[-0.2404,  0.0512],\n",
      "        [-0.2545,  0.0577]], grad_fn=<AddmmBackward0>)\n",
      "‚û°Ô∏è FORWARD PASS ENDED\n",
      "==================================================\n",
      " Loss: 0.8554551601409912\n",
      " Backpropagation done\n",
      " Weights updated\n",
      "\n",
      " Batch 2\n",
      "Batch sequences shape: torch.Size([2, 4])\n",
      "Batch labels: tensor([0, 0])\n",
      "‚û°Ô∏è FORWARD PASS STARTED\n",
      "Input x shape (batch_size, seq_len): torch.Size([2, 4])\n",
      "Input x: tensor([[2., 1., 3., 0.],\n",
      "        [1., 3., 2., 6.]])\n",
      "After unsqueeze\n",
      "x shape (batch_size, seq_len, input_size): torch.Size([2, 4, 1])\n",
      "üîÅ RNN OUTPUT\n",
      "rnn_output shape (batch_size, seq_len, hidden_size): torch.Size([2, 4, 16])\n",
      "hidden_state shape (num_layers, batch_size, hidden_size): torch.Size([1, 2, 16])\n",
      "üß† Last hidden state\n",
      "last_hidden shape (batch_size, hidden_size): torch.Size([2, 16])\n",
      "üì§ Output logits\n",
      "output shape (batch_size, num_classes): torch.Size([2, 2])\n",
      "output: tensor([[-0.2173, -0.0048],\n",
      "        [-0.1087, -0.0330]], grad_fn=<AddmmBackward0>)\n",
      "‚û°Ô∏è FORWARD PASS ENDED\n",
      "==================================================\n",
      " Loss: 0.7683820724487305\n",
      " Backpropagation done\n",
      " Weights updated\n",
      "\n",
      " Batch 3\n",
      "Batch sequences shape: torch.Size([1, 4])\n",
      "Batch labels: tensor([1])\n",
      "‚û°Ô∏è FORWARD PASS STARTED\n",
      "Input x shape (batch_size, seq_len): torch.Size([1, 4])\n",
      "Input x: tensor([[1., 2., 4., 0.]])\n",
      "After unsqueeze\n",
      "x shape (batch_size, seq_len, input_size): torch.Size([1, 4, 1])\n",
      "üîÅ RNN OUTPUT\n",
      "rnn_output shape (batch_size, seq_len, hidden_size): torch.Size([1, 4, 16])\n",
      "hidden_state shape (num_layers, batch_size, hidden_size): torch.Size([1, 1, 16])\n",
      "üß† Last hidden state\n",
      "last_hidden shape (batch_size, hidden_size): torch.Size([1, 16])\n",
      "üì§ Output logits\n",
      "output shape (batch_size, num_classes): torch.Size([1, 2])\n",
      "output: tensor([[-0.1692, -0.0835]], grad_fn=<AddmmBackward0>)\n",
      "‚û°Ô∏è FORWARD PASS ENDED\n",
      "==================================================\n",
      " Loss: 0.6512129902839661\n",
      " Backpropagation done\n",
      " Weights updated\n",
      "\n",
      " EPOCH 2/2\n",
      "============================================================\n",
      "\n",
      " Batch 1\n",
      "Batch sequences shape: torch.Size([2, 4])\n",
      "Batch labels: tensor([0, 0])\n",
      "‚û°Ô∏è FORWARD PASS STARTED\n",
      "Input x shape (batch_size, seq_len): torch.Size([2, 4])\n",
      "Input x: tensor([[1., 3., 2., 6.],\n",
      "        [1., 2., 3., 0.]])\n",
      "After unsqueeze\n",
      "x shape (batch_size, seq_len, input_size): torch.Size([2, 4, 1])\n",
      "üîÅ RNN OUTPUT\n",
      "rnn_output shape (batch_size, seq_len, hidden_size): torch.Size([2, 4, 16])\n",
      "hidden_state shape (num_layers, batch_size, hidden_size): torch.Size([1, 2, 16])\n",
      "üß† Last hidden state\n",
      "last_hidden shape (batch_size, hidden_size): torch.Size([2, 16])\n",
      "üì§ Output logits\n",
      "output shape (batch_size, num_classes): torch.Size([2, 2])\n",
      "output: tensor([[-0.0379, -0.1741],\n",
      "        [-0.1669, -0.0879]], grad_fn=<AddmmBackward0>)\n",
      "‚û°Ô∏è FORWARD PASS ENDED\n",
      "==================================================\n",
      " Loss: 0.6803718209266663\n",
      " Backpropagation done\n",
      " Weights updated\n",
      "\n",
      " Batch 2\n",
      "Batch sequences shape: torch.Size([2, 4])\n",
      "Batch labels: tensor([1, 0])\n",
      "‚û°Ô∏è FORWARD PASS STARTED\n",
      "Input x shape (batch_size, seq_len): torch.Size([2, 4])\n",
      "Input x: tensor([[1., 2., 4., 0.],\n",
      "        [2., 1., 3., 0.]])\n",
      "After unsqueeze\n",
      "x shape (batch_size, seq_len, input_size): torch.Size([2, 4, 1])\n",
      "üîÅ RNN OUTPUT\n",
      "rnn_output shape (batch_size, seq_len, hidden_size): torch.Size([2, 4, 16])\n",
      "hidden_state shape (num_layers, batch_size, hidden_size): torch.Size([1, 2, 16])\n",
      "üß† Last hidden state\n",
      "last_hidden shape (batch_size, hidden_size): torch.Size([2, 16])\n",
      "üì§ Output logits\n",
      "output shape (batch_size, num_classes): torch.Size([2, 2])\n",
      "output: tensor([[-0.1386, -0.1284],\n",
      "        [-0.1517, -0.1139]], grad_fn=<AddmmBackward0>)\n",
      "‚û°Ô∏è FORWARD PASS ENDED\n",
      "==================================================\n",
      " Loss: 0.700127363204956\n",
      " Backpropagation done\n",
      " Weights updated\n",
      "\n",
      " Batch 3\n",
      "Batch sequences shape: torch.Size([1, 4])\n",
      "Batch labels: tensor([0])\n",
      "‚û°Ô∏è FORWARD PASS STARTED\n",
      "Input x shape (batch_size, seq_len): torch.Size([1, 4])\n",
      "Input x: tensor([[4., 1., 5., 0.]])\n",
      "After unsqueeze\n",
      "x shape (batch_size, seq_len, input_size): torch.Size([1, 4, 1])\n",
      "üîÅ RNN OUTPUT\n",
      "rnn_output shape (batch_size, seq_len, hidden_size): torch.Size([1, 4, 16])\n",
      "hidden_state shape (num_layers, batch_size, hidden_size): torch.Size([1, 1, 16])\n",
      "üß† Last hidden state\n",
      "last_hidden shape (batch_size, hidden_size): torch.Size([1, 16])\n",
      "üì§ Output logits\n",
      "output shape (batch_size, num_classes): torch.Size([1, 2])\n",
      "output: tensor([[-0.1137, -0.1620]], grad_fn=<AddmmBackward0>)\n",
      "‚û°Ô∏è FORWARD PASS ENDED\n",
      "==================================================\n",
      " Loss: 0.6692805290222168\n",
      " Backpropagation done\n",
      " Weights updated\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 2  # keep small because prints are heavy\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\n EPOCH {epoch+1}/{num_epochs}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    for batch_index, (sequences_batch, labels_batch) in enumerate(dataloader):\n",
    "        print(f\"\\n Batch {batch_index + 1}\")\n",
    "        print(\"Batch sequences shape:\", sequences_batch.shape)\n",
    "        print(\"Batch labels:\", labels_batch)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(sequences_batch)\n",
    "\n",
    "        loss = criterion(outputs, labels_batch)\n",
    "        print(\" Loss:\", loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "        print(\" Backpropagation done\")\n",
    "\n",
    "        optimizer.step()\n",
    "        print(\" Weights updated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a786d19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
